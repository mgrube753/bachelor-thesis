
@online{openai_introducing_2022,
	title = {Introducing {ChatGPT}},
	url = {https://openai.com/index/chatgpt/},
	abstract = {We’ve trained a model called {ChatGPT} which interacts in a conversational way. The dialogue format makes it possible for {ChatGPT} to answer followup questions, admit its mistakes, challenge incorrect premises, and reject inappropriate requests.},
	author = {{OpenAI}},
	urldate = {2025-04-24},
	date = {2022-11-30},
	langid = {american},
	file = {Snapshot:/home/malde/Zotero/storage/AUC8IH66/chatgpt.html:text/html},
}

@misc{openai_gpt-4_2024,
	title = {{GPT}-4 Technical Report},
	url = {http://arxiv.org/abs/2303.08774},
	doi = {10.48550/arXiv.2303.08774},
	abstract = {We report the development of {GPT}-4, a large-scale, multimodal model which can accept image and text inputs and produce text outputs. While less capable than humans in many real-world scenarios, {GPT}-4 exhibits human-level performance on various professional and academic benchmarks, including passing a simulated bar exam with a score around the top 10\% of test takers. {GPT}-4 is a Transformer-based model pre-trained to predict the next token in a document. The post-training alignment process results in improved performance on measures of factuality and adherence to desired behavior. A core component of this project was developing infrastructure and optimization methods that behave predictably across a wide range of scales. This allowed us to accurately predict some aspects of {GPT}-4's performance based on models trained with no more than 1/1,000th the compute of {GPT}-4.},
	number = {{arXiv}:2303.08774},
	publisher = {{arXiv}},
	author = {{OpenAI} and Achiam, Josh and Adler, Steven and Agarwal, Sandhini and Ahmad, Lama and Akkaya, Ilge and Aleman, Florencia Leoni and Almeida, Diogo and Altenschmidt, Janko and Altman, Sam and Anadkat, Shyamal and Avila, Red and Babuschkin, Igor and Balaji, Suchir and Balcom, Valerie and Baltescu, Paul and Bao, Haiming and Bavarian, Mohammad and Belgum, Jeff and Bello, Irwan and Berdine, Jake and Bernadett-Shapiro, Gabriel and Berner, Christopher and Bogdonoff, Lenny and Boiko, Oleg and Boyd, Madelaine and Brakman, Anna-Luisa and Brockman, Greg and Brooks, Tim and Brundage, Miles and Button, Kevin and Cai, Trevor and Campbell, Rosie and Cann, Andrew and Carey, Brittany and Carlson, Chelsea and Carmichael, Rory and Chan, Brooke and Chang, Che and Chantzis, Fotis and Chen, Derek and Chen, Sully and Chen, Ruby and Chen, Jason and Chen, Mark and Chess, Ben and Cho, Chester and Chu, Casey and Chung, Hyung Won and Cummings, Dave and Currier, Jeremiah and Dai, Yunxing and Decareaux, Cory and Degry, Thomas and Deutsch, Noah and Deville, Damien and Dhar, Arka and Dohan, David and Dowling, Steve and Dunning, Sheila and Ecoffet, Adrien and Eleti, Atty and Eloundou, Tyna and Farhi, David and Fedus, Liam and Felix, Niko and Fishman, Simón Posada and Forte, Juston and Fulford, Isabella and Gao, Leo and Georges, Elie and Gibson, Christian and Goel, Vik and Gogineni, Tarun and Goh, Gabriel and Gontijo-Lopes, Rapha and Gordon, Jonathan and Grafstein, Morgan and Gray, Scott and Greene, Ryan and Gross, Joshua and Gu, Shixiang Shane and Guo, Yufei and Hallacy, Chris and Han, Jesse and Harris, Jeff and He, Yuchen and Heaton, Mike and Heidecke, Johannes and Hesse, Chris and Hickey, Alan and Hickey, Wade and Hoeschele, Peter and Houghton, Brandon and Hsu, Kenny and Hu, Shengli and Hu, Xin and Huizinga, Joost and Jain, Shantanu and Jain, Shawn and Jang, Joanne and Jiang, Angela and Jiang, Roger and Jin, Haozhun and Jin, Denny and Jomoto, Shino and Jonn, Billie and Jun, Heewoo and Kaftan, Tomer and Kaiser, Łukasz and Kamali, Ali and Kanitscheider, Ingmar and Keskar, Nitish Shirish and Khan, Tabarak and Kilpatrick, Logan and Kim, Jong Wook and Kim, Christina and Kim, Yongjik and Kirchner, Jan Hendrik and Kiros, Jamie and Knight, Matt and Kokotajlo, Daniel and Kondraciuk, Łukasz and Kondrich, Andrew and Konstantinidis, Aris and Kosic, Kyle and Krueger, Gretchen and Kuo, Vishal and Lampe, Michael and Lan, Ikai and Lee, Teddy and Leike, Jan and Leung, Jade and Levy, Daniel and Li, Chak Ming and Lim, Rachel and Lin, Molly and Lin, Stephanie and Litwin, Mateusz and Lopez, Theresa and Lowe, Ryan and Lue, Patricia and Makanju, Anna and Malfacini, Kim and Manning, Sam and Markov, Todor and Markovski, Yaniv and Martin, Bianca and Mayer, Katie and Mayne, Andrew and {McGrew}, Bob and {McKinney}, Scott Mayer and {McLeavey}, Christine and {McMillan}, Paul and {McNeil}, Jake and Medina, David and Mehta, Aalok and Menick, Jacob and Metz, Luke and Mishchenko, Andrey and Mishkin, Pamela and Monaco, Vinnie and Morikawa, Evan and Mossing, Daniel and Mu, Tong and Murati, Mira and Murk, Oleg and Mély, David and Nair, Ashvin and Nakano, Reiichiro and Nayak, Rajeev and Neelakantan, Arvind and Ngo, Richard and Noh, Hyeonwoo and Ouyang, Long and O'Keefe, Cullen and Pachocki, Jakub and Paino, Alex and Palermo, Joe and Pantuliano, Ashley and Parascandolo, Giambattista and Parish, Joel and Parparita, Emy and Passos, Alex and Pavlov, Mikhail and Peng, Andrew and Perelman, Adam and Peres, Filipe de Avila Belbute and Petrov, Michael and Pinto, Henrique Ponde de Oliveira and Michael and Pokorny and Pokrass, Michelle and Pong, Vitchyr H. and Powell, Tolly and Power, Alethea and Power, Boris and Proehl, Elizabeth and Puri, Raul and Radford, Alec and Rae, Jack and Ramesh, Aditya and Raymond, Cameron and Real, Francis and Rimbach, Kendra and Ross, Carl and Rotsted, Bob and Roussez, Henri and Ryder, Nick and Saltarelli, Mario and Sanders, Ted and Santurkar, Shibani and Sastry, Girish and Schmidt, Heather and Schnurr, David and Schulman, John and Selsam, Daniel and Sheppard, Kyla and Sherbakov, Toki and Shieh, Jessica and Shoker, Sarah and Shyam, Pranav and Sidor, Szymon and Sigler, Eric and Simens, Maddie and Sitkin, Jordan and Slama, Katarina and Sohl, Ian and Sokolowsky, Benjamin and Song, Yang and Staudacher, Natalie and Such, Felipe Petroski and Summers, Natalie and Sutskever, Ilya and Tang, Jie and Tezak, Nikolas and Thompson, Madeleine B. and Tillet, Phil and Tootoonchian, Amin and Tseng, Elizabeth and Tuggle, Preston and Turley, Nick and Tworek, Jerry and Uribe, Juan Felipe Cerón and Vallone, Andrea and Vijayvergiya, Arun and Voss, Chelsea and Wainwright, Carroll and Wang, Justin Jay and Wang, Alvin and Wang, Ben and Ward, Jonathan and Wei, Jason and Weinmann, C. J. and Welihinda, Akila and Welinder, Peter and Weng, Jiayi and Weng, Lilian and Wiethoff, Matt and Willner, Dave and Winter, Clemens and Wolrich, Samuel and Wong, Hannah and Workman, Lauren and Wu, Sherwin and Wu, Jeff and Wu, Michael and Xiao, Kai and Xu, Tao and Yoo, Sarah and Yu, Kevin and Yuan, Qiming and Zaremba, Wojciech and Zellers, Rowan and Zhang, Chong and Zhang, Marvin and Zhao, Shengjia and Zheng, Tianhao and Zhuang, Juntang and Zhuk, William and Zoph, Barret},
	urldate = {2025-04-24},
	date = {2024-03-04},
	eprinttype = {arxiv},
	eprint = {2303.08774 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence},
	file = {Preprint PDF:/home/malde/Zotero/storage/KMGB3TIT/OpenAI et al. - 2024 - GPT-4 Technical Report.pdf:application/pdf;Snapshot:/home/malde/Zotero/storage/IFBNHFL6/2303.html:text/html},
}

@misc{lewis_bart_2019,
	title = {{BART}: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension},
	url = {http://arxiv.org/abs/1910.13461},
	doi = {10.48550/arXiv.1910.13461},
	shorttitle = {{BART}},
	abstract = {We present {BART}, a denoising autoencoder for pretraining sequence-to-sequence models. {BART} is trained by (1) corrupting text with an arbitrary noising function, and (2) learning a model to reconstruct the original text. It uses a standard Tranformer-based neural machine translation architecture which, despite its simplicity, can be seen as generalizing {BERT} (due to the bidirectional encoder), {GPT} (with the left-to-right decoder), and many other more recent pretraining schemes. We evaluate a number of noising approaches, ﬁnding the best performance by both randomly shufﬂing the order of the original sentences and using a novel in-ﬁlling scheme, where spans of text are replaced with a single mask token. {BART} is particularly effective when ﬁne tuned for text generation but also works well for comprehension tasks. It matches the performance of {RoBERTa} with comparable training resources on {GLUE} and {SQuAD}, achieves new stateof-the-art results on a range of abstractive dialogue, question answering, and summarization tasks, with gains of up to 6 {ROUGE}. {BART} also provides a 1.1 {BLEU} increase over a back-translation system for machine translation, with only target language pretraining. We also report ablation experiments that replicate other pretraining schemes within the {BART} framework, to better measure which factors most inﬂuence end-task performance.},
	number = {{arXiv}:1910.13461},
	publisher = {{arXiv}},
	author = {Lewis, Mike and Liu, Yinhan and Goyal, Naman and Ghazvininejad, Marjan and Mohamed, Abdelrahman and Levy, Omer and Stoyanov, Ves and Zettlemoyer, Luke},
	urldate = {2025-04-24},
	date = {2019-10-29},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1910.13461 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {PDF:/home/malde/Zotero/storage/HCTVGJH2/Lewis et al. - 2019 - BART Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and.pdf:application/pdf},
}

@misc{raffel_exploring_2023,
	title = {Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer},
	url = {http://arxiv.org/abs/1910.10683},
	doi = {10.48550/arXiv.1910.10683},
	abstract = {Transfer learning, where a model is first pre-trained on a data-rich task before being fine-tuned on a downstream task, has emerged as a powerful technique in natural language processing ({NLP}). The effectiveness of transfer learning has given rise to a diversity of approaches, methodology, and practice. In this paper, we explore the landscape of transfer learning techniques for {NLP} by introducing a unified framework that converts all text-based language problems into a text-to-text format. Our systematic study compares pre-training objectives, architectures, unlabeled data sets, transfer approaches, and other factors on dozens of language understanding tasks. By combining the insights from our exploration with scale and our new ``Colossal Clean Crawled Corpus'', we achieve state-of-the-art results on many benchmarks covering summarization, question answering, text classification, and more. To facilitate future work on transfer learning for {NLP}, we release our data set, pre-trained models, and code.},
	number = {{arXiv}:1910.10683},
	publisher = {{arXiv}},
	author = {Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J.},
	urldate = {2025-04-24},
	date = {2023-09-19},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1910.10683 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {PDF:/home/malde/Zotero/storage/4S9YRFC2/Raffel et al. - 2023 - Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer.pdf:application/pdf},
}

@misc{liu_roberta_2019,
	title = {{RoBERTa}: A Robustly Optimized {BERT} Pretraining Approach},
	url = {http://arxiv.org/abs/1907.11692},
	doi = {10.48550/arXiv.1907.11692},
	shorttitle = {{RoBERTa}},
	abstract = {Language model pretraining has led to significant performance gains but careful comparison between different approaches is challenging. Training is computationally expensive, often done on private datasets of different sizes, and, as we will show, hyperparameter choices have significant impact on the final results. We present a replication study of {BERT} pretraining (Devlin et al., 2019) that carefully measures the impact of many key hyperparameters and training data size. We find that {BERT} was significantly undertrained, and can match or exceed the performance of every model published after it. Our best model achieves state-of-the-art results on {GLUE}, {RACE} and {SQuAD}. These results highlight the importance of previously overlooked design choices, and raise questions about the source of recently reported improvements. We release our models and code.},
	number = {{arXiv}:1907.11692},
	publisher = {{arXiv}},
	author = {Liu, Yinhan and Ott, Myle and Goyal, Naman and Du, Jingfei and Joshi, Mandar and Chen, Danqi and Levy, Omer and Lewis, Mike and Zettlemoyer, Luke and Stoyanov, Veselin},
	urldate = {2025-04-24},
	date = {2019-07-26},
	eprinttype = {arxiv},
	eprint = {1907.11692 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {Preprint PDF:/home/malde/Zotero/storage/DQX98PPR/Liu et al. - 2019 - RoBERTa A Robustly Optimized BERT Pretraining Approach.pdf:application/pdf;Snapshot:/home/malde/Zotero/storage/TSEN4MET/1907.html:text/html},
}

@misc{lan_albert_2020,
	title = {{ALBERT}: A Lite {BERT} for Self-supervised Learning of Language Representations},
	url = {http://arxiv.org/abs/1909.11942},
	doi = {10.48550/arXiv.1909.11942},
	shorttitle = {{ALBERT}},
	abstract = {Increasing model size when pretraining natural language representations often results in improved performance on downstream tasks. However, at some point further model increases become harder due to {GPU}/{TPU} memory limitations and longer training times. To address these problems, we present two parameter-reduction techniques to lower memory consumption and increase the training speed of {BERT}. Comprehensive empirical evidence shows that our proposed methods lead to models that scale much better compared to the original {BERT}. We also use a self-supervised loss that focuses on modeling inter-sentence coherence, and show it consistently helps downstream tasks with multi-sentence inputs. As a result, our best model establishes new state-of-the-art results on the {GLUE}, {RACE}, and {\textbackslash}squad benchmarks while having fewer parameters compared to {BERT}-large. The code and the pretrained models are available at https://github.com/google-research/{ALBERT}.},
	number = {{arXiv}:1909.11942},
	publisher = {{arXiv}},
	author = {Lan, Zhenzhong and Chen, Mingda and Goodman, Sebastian and Gimpel, Kevin and Sharma, Piyush and Soricut, Radu},
	urldate = {2025-04-24},
	date = {2020-02-09},
	eprinttype = {arxiv},
	eprint = {1909.11942 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence},
	file = {Preprint PDF:/home/malde/Zotero/storage/ST38UJDP/Lan et al. - 2020 - ALBERT A Lite BERT for Self-supervised Learning of Language Representations.pdf:application/pdf;Snapshot:/home/malde/Zotero/storage/WBJ5DTRL/1909.html:text/html},
}

@misc{zhang_ernie_2019,
	title = {{ERNIE}: Enhanced Language Representation with Informative Entities},
	url = {http://arxiv.org/abs/1905.07129},
	doi = {10.48550/arXiv.1905.07129},
	shorttitle = {{ERNIE}},
	abstract = {Neural language representation models such as {BERT} pre-trained on large-scale corpora can well capture rich semantic patterns from plain text, and be ﬁne-tuned to consistently improve the performance of various {NLP} tasks. However, the existing pre-trained language models rarely consider incorporating knowledge graphs ({KGs}), which can provide rich structured knowledge facts for better language understanding. We argue that informative entities in {KGs} can enhance language representation with external knowledge. In this paper, we utilize both large-scale textual corpora and {KGs} to train an enhanced language representation model ({ERNIE}), which can take full advantage of lexical, syntactic, and knowledge information simultaneously. The experimental results have demonstrated that {ERNIE} achieves signiﬁcant improvements on various knowledge-driven tasks, and meanwhile is comparable with the state-of-the-art model {BERT} on other common {NLP} tasks. The source code and experiment details of this paper can be obtained from https:// github.com/thunlp/{ERNIE}.},
	number = {{arXiv}:1905.07129},
	publisher = {{arXiv}},
	author = {Zhang, Zhengyan and Han, Xu and Liu, Zhiyuan and Jiang, Xin and Sun, Maosong and Liu, Qun},
	urldate = {2025-04-24},
	date = {2019-06-04},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1905.07129 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {PDF:/home/malde/Zotero/storage/FV7M3KVQ/Zhang et al. - 2019 - ERNIE Enhanced Language Representation with Informative Entities.pdf:application/pdf},
}

@misc{devlin_bert_2019,
	title = {{BERT}: Pre-training of Deep Bidirectional Transformers for Language Understanding},
	url = {http://arxiv.org/abs/1810.04805},
	doi = {10.48550/arXiv.1810.04805},
	shorttitle = {{BERT}},
	abstract = {We introduce a new language representation model called {BERT}, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, {BERT} is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained {BERT} model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. {BERT} is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the {GLUE} score to 80.5\% (7.7\% point absolute improvement), {MultiNLI} accuracy to 86.7\% (4.6\% absolute improvement), {SQuAD} v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and {SQuAD} v2.0 Test F1 to 83.1 (5.1 point absolute improvement).},
	number = {{arXiv}:1810.04805},
	publisher = {{arXiv}},
	author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
	urldate = {2025-04-24},
	date = {2019-05-24},
	eprinttype = {arxiv},
	eprint = {1810.04805 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {Preprint PDF:/home/malde/Zotero/storage/TDB33C8P/Devlin et al. - 2019 - BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf:application/pdf;Snapshot:/home/malde/Zotero/storage/HVQASIJA/1810.html:text/html},
}

@article{krathwohl_revision_2002,
	title = {A Revision of Bloom's Taxonomy: An Overview},
	volume = {41},
	issn = {0040-5841, 1543-0421},
	url = {https://www.tandfonline.com/doi/full/10.1207/s15430421tip4104_2},
	doi = {10.1207/s15430421tip4104_2},
	shorttitle = {A Revision of Bloom's Taxonomy},
	pages = {212--218},
	number = {4},
	journaltitle = {Theory Into Practice},
	shortjournal = {Theory Into Practice},
	author = {Krathwohl, David R.},
	urldate = {2025-04-24},
	date = {2002-11-01},
	langid = {english},
	file = {Krathwohl-RevisionBloomsTaxonomy-2002:/home/malde/Zotero/storage/YW78JW2M/Krathwohl-RevisionBloomsTaxonomy-2002.pdf:application/pdf;Submitted Version:/home/malde/Zotero/storage/H387D9QC/Krathwohl - 2002 - A Revision of Bloom's Taxonomy An Overview.pdf:application/pdf},
}

@book{krathwohl_taxonomy_1964,
	title = {Taxonomy of educational objectives},
	volume = {2},
	publisher = {David {McKay} Company},
	author = {Krathwohl, David R and Bloom, Benjamin and Masia, Bertram B},
	date = {1964},
}

@misc{shazeer_outrageously_2017,
	title = {Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer},
	url = {http://arxiv.org/abs/1701.06538},
	doi = {10.48550/arXiv.1701.06538},
	shorttitle = {Outrageously Large Neural Networks},
	abstract = {The capacity of a neural network to absorb information is limited by its number of parameters. Conditional computation, where parts of the network are active on a per-example basis, has been proposed in theory as a way of dramatically increasing model capacity without a proportional increase in computation. In practice, however, there are significant algorithmic and performance challenges. In this work, we address these challenges and finally realize the promise of conditional computation, achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern {GPU} clusters. We introduce a Sparsely-Gated Mixture-of-Experts layer ({MoE}), consisting of up to thousands of feed-forward sub-networks. A trainable gating network determines a sparse combination of these experts to use for each example. We apply the {MoE} to the tasks of language modeling and machine translation, where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora. We present model architectures in which a {MoE} with up to 137 billion parameters is applied convolutionally between stacked {LSTM} layers. On large language modeling and machine translation benchmarks, these models achieve significantly better results than state-of-the-art at lower computational cost.},
	number = {{arXiv}:1701.06538},
	publisher = {{arXiv}},
	author = {Shazeer, Noam and Mirhoseini, Azalia and Maziarz, Krzysztof and Davis, Andy and Le, Quoc and Hinton, Geoffrey and Dean, Jeff},
	urldate = {2025-04-27},
	date = {2017-01-23},
	eprinttype = {arxiv},
	eprint = {1701.06538 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Neural and Evolutionary Computing},
	file = {Preprint PDF:/home/malde/Zotero/storage/PDC4U4BB/Shazeer et al. - 2017 - Outrageously Large Neural Networks The Sparsely-Gated Mixture-of-Experts Layer.pdf:application/pdf;Snapshot:/home/malde/Zotero/storage/9Y4JIM88/1701.html:text/html},
}

@misc{touvron_llama_2023,
	title = {{LLaMA}: Open and Efficient Foundation Language Models},
	url = {http://arxiv.org/abs/2302.13971},
	doi = {10.48550/arXiv.2302.13971},
	shorttitle = {{LLaMA}},
	abstract = {We introduce {LLaMA}, a collection of foundation language models ranging from 7B to 65B parameters. We train our models on trillions of tokens, and show that it is possible to train state-of-the-art models using publicly available datasets exclusively, without resorting to proprietary and inaccessible datasets. In particular, {LLaMA}-13B outperforms {GPT}-3 (175B) on most benchmarks, and {LLaMA}65B is competitive with the best models, Chinchilla-70B and {PaLM}-540B. We release all our models to the research community1.},
	number = {{arXiv}:2302.13971},
	publisher = {{arXiv}},
	author = {Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and Martinet, Xavier and Lachaux, Marie-Anne and Lacroix, Timothée and Rozière, Baptiste and Goyal, Naman and Hambro, Eric and Azhar, Faisal and Rodriguez, Aurelien and Joulin, Armand and Grave, Edouard and Lample, Guillaume},
	urldate = {2025-04-26},
	date = {2023-02-27},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2302.13971 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {PDF:/home/malde/Zotero/storage/CG3847PQ/Touvron et al. - 2023 - LLaMA Open and Efficient Foundation Language Models.pdf:application/pdf},
}

@article{radford_improving_2018,
	title = {Improving language understanding by generative pre-training},
	url = {https://www.mikecaptain.com/resources/pdf/GPT-1.pdf},
	author = {Radford, Alec and Narasimhan, Karthik and Salimans, Tim and Sutskever, Ilya and {others}},
	urldate = {2025-04-24},
	date = {2018},
	note = {Publisher: San Francisco, {CA}, {USA}},
	file = {PDF:/home/malde/Zotero/storage/5LRUHEW4/Radford et al. - Improving Language Understanding by Generative Pre-Training.pdf:application/pdf},
}

@misc{touvron_llama_2023-1,
	title = {Llama 2: Open Foundation and Fine-Tuned Chat Models},
	url = {http://arxiv.org/abs/2307.09288},
	doi = {10.48550/arXiv.2307.09288},
	shorttitle = {Llama 2},
	abstract = {In this work, we develop and release Llama 2, a collection of pretrained and fine-tuned large language models ({LLMs}) ranging in scale from 7 billion to 70 billion parameters. Our fine-tuned {LLMs}, called Llama 2-Chat, are optimized for dialogue use cases. Our models outperform open-source chat models on most benchmarks we tested, and based on our human evaluations for helpfulness and safety, may be a suitable substitute for closedsource models. We provide a detailed description of our approach to fine-tuning and safety improvements of Llama 2-Chat in order to enable the community to build on our work and contribute to the responsible development of {LLMs}.},
	number = {{arXiv}:2307.09288},
	publisher = {{arXiv}},
	author = {Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert, Peter and Almahairi, Amjad and Babaei, Yasmine and Bashlykov, Nikolay and Batra, Soumya and Bhargava, Prajjwal and Bhosale, Shruti and Bikel, Dan and Blecher, Lukas and Ferrer, Cristian Canton and Chen, Moya and Cucurull, Guillem and Esiobu, David and Fernandes, Jude and Fu, Jeremy and Fu, Wenyin and Fuller, Brian and Gao, Cynthia and Goswami, Vedanuj and Goyal, Naman and Hartshorn, Anthony and Hosseini, Saghar and Hou, Rui and Inan, Hakan and Kardas, Marcin and Kerkez, Viktor and Khabsa, Madian and Kloumann, Isabel and Korenev, Artem and Koura, Punit Singh and Lachaux, Marie-Anne and Lavril, Thibaut and Lee, Jenya and Liskovich, Diana and Lu, Yinghai and Mao, Yuning and Martinet, Xavier and Mihaylov, Todor and Mishra, Pushkar and Molybog, Igor and Nie, Yixin and Poulton, Andrew and Reizenstein, Jeremy and Rungta, Rashi and Saladi, Kalyan and Schelten, Alan and Silva, Ruan and Smith, Eric Michael and Subramanian, Ranjan and Tan, Xiaoqing Ellen and Tang, Binh and Taylor, Ross and Williams, Adina and Kuan, Jian Xiang and Xu, Puxin and Yan, Zheng and Zarov, Iliyan and Zhang, Yuchen and Fan, Angela and Kambadur, Melanie and Narang, Sharan and Rodriguez, Aurelien and Stojnic, Robert and Edunov, Sergey and Scialom, Thomas},
	urldate = {2025-04-28},
	date = {2023-07-19},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2307.09288 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence},
	file = {PDF:/home/malde/Zotero/storage/TTPWW4LG/Touvron et al. - 2023 - Llama 2 Open Foundation and Fine-Tuned Chat Models.pdf:application/pdf},
}

@misc{brown_language_2020,
	title = {Language Models are Few-Shot Learners},
	url = {http://arxiv.org/abs/2005.14165},
	doi = {10.48550/arXiv.2005.14165},
	abstract = {Recent work has demonstrated substantial gains on many {NLP} tasks and benchmarks by pre-training on a large corpus of text followed by ﬁne-tuning on a speciﬁc task. While typically task-agnostic in architecture, this method still requires task-speciﬁc ﬁne-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions – something which current {NLP} systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art ﬁnetuning approaches. Speciﬁcally, we train {GPT}-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, {GPT}-3 is applied without any gradient updates or ﬁne-tuning, with tasks and few-shot demonstrations speciﬁed purely via text interaction with the model. {GPT}-3 achieves strong performance on many {NLP} datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-ﬂy reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where {GPT}-3’s few-shot learning still struggles, as well as some datasets where {GPT}-3 faces methodological issues related to training on large web corpora. Finally, we ﬁnd that {GPT}-3 can generate samples of news articles which human evaluators have difﬁculty distinguishing from articles written by humans. We discuss broader societal impacts of this ﬁnding and of {GPT}-3 in general.},
	number = {{arXiv}:2005.14165},
	publisher = {{arXiv}},
	author = {Brown, Tom B. and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel M. and Wu, Jeffrey and Winter, Clemens and Hesse, Christopher and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and {McCandlish}, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
	urldate = {2025-04-28},
	date = {2020-07-22},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2005.14165 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {PDF:/home/malde/Zotero/storage/DTLZUDP2/Brown et al. - 2020 - Language Models are Few-Shot Learners.pdf:application/pdf},
}

@inproceedings{papineni_bleu_2001,
	location = {Philadelphia, Pennsylvania},
	title = {{BLEU}: a method for automatic evaluation of machine translation},
	url = {http://portal.acm.org/citation.cfm?doid=1073083.1073135},
	doi = {10.3115/1073083.1073135},
	shorttitle = {{BLEU}},
	eventtitle = {the 40th Annual Meeting},
	pages = {311},
	booktitle = {Proceedings of the 40th Annual Meeting on Association for Computational Linguistics  - {ACL} '02},
	publisher = {Association for Computational Linguistics},
	author = {Papineni, Kishore and Roukos, Salim and Ward, Todd and Zhu, Wei-Jing},
	urldate = {2025-04-29},
	date = {2001},
	langid = {english},
	file = {PDF:/home/malde/Zotero/storage/6QYVICX8/Papineni et al. - 2001 - BLEU a method for automatic evaluation of machine translation.pdf:application/pdf},
}

@inproceedings{lin_rouge_2004,
	location = {Barcelona, Spain},
	title = {{ROUGE}: A Package for Automatic Evaluation of Summaries},
	url = {https://aclanthology.org/W04-1013/},
	shorttitle = {{ROUGE}},
	pages = {74--81},
	booktitle = {Text Summarization Branches Out},
	publisher = {Association for Computational Linguistics},
	author = {Lin, Chin-Yew},
	urldate = {2025-04-29},
	date = {2004-07},
	file = {Full Text PDF:/home/malde/Zotero/storage/3U367QSW/Lin - 2004 - ROUGE A Package for Automatic Evaluation of Summaries.pdf:application/pdf},
}

@inproceedings{banerjee_meteor_2005,
	location = {Ann Arbor, Michigan},
	title = {{METEOR}: An Automatic Metric for {MT} Evaluation with Improved Correlation with Human Judgments},
	url = {https://aclanthology.org/W05-0909/},
	pages = {65--72},
	booktitle = {Proceedings of the {ACL} Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization},
	publisher = {Association for Computational Linguistics},
	author = {Banerjee, Satanjeev and Lavie, Alon},
	editor = {Goldstein, Jade and Lavie, Alon and Lin, Chin-Yew and Voss, Clare},
	urldate = {2025-04-29},
	date = {2005-06},
	file = {PDF:/home/malde/Zotero/storage/XX5A3MFV/Proceedings of the....pdf:application/pdf},
}

@misc{li_diversity-promoting_2016,
	title = {A Diversity-Promoting Objective Function for Neural Conversation Models},
	url = {http://arxiv.org/abs/1510.03055},
	doi = {10.48550/arXiv.1510.03055},
	abstract = {Sequence-to-sequence neural network models for generation of conversational responses tend to generate safe, commonplace responses (e.g., I don’t know) regardless of the input. We suggest that the traditional objective function, i.e., the likelihood of output (response) given input (message) is unsuited to response generation tasks. Instead we propose using Maximum Mutual Information ({MMI}) as the objective function in neural models. Experimental results demonstrate that the proposed {MMI} models produce more diverse, interesting, and appropriate responses, yielding substantive gains in {BLEU} scores on two conversational datasets and in human evaluations.},
	number = {{arXiv}:1510.03055},
	publisher = {{arXiv}},
	author = {Li, Jiwei and Galley, Michel and Brockett, Chris and Gao, Jianfeng and Dolan, Bill},
	urldate = {2025-04-29},
	date = {2016-06-10},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1510.03055 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {PDF:/home/malde/Zotero/storage/GZJWB5MA/Li et al. - 2016 - A Diversity-Promoting Objective Function for Neural Conversation Models.pdf:application/pdf},
}

@misc{zhang_bertscore_2020,
	title = {{BERTScore}: Evaluating Text Generation with {BERT}},
	url = {http://arxiv.org/abs/1904.09675},
	doi = {10.48550/arXiv.1904.09675},
	shorttitle = {{BERTScore}},
	abstract = {We propose {BERTScore}, an automatic evaluation metric for text generation. Analogously to common metrics, {BERTScore} computes a similarity score for each token in the candidate sentence with each token in the reference sentence. However, instead of exact matches, we compute token similarity using contextual embeddings. We evaluate using the outputs of 363 machine translation and image captioning systems. {BERTScore} correlates better with human judgments and provides stronger model selection performance than existing metrics. Finally, we use an adversarial paraphrase detection task to show that {BERTScore} is more robust to challenging examples when compared to existing metrics.},
	number = {{arXiv}:1904.09675},
	publisher = {{arXiv}},
	author = {Zhang, Tianyi and Kishore, Varsha and Wu, Felix and Weinberger, Kilian Q. and Artzi, Yoav},
	urldate = {2025-04-29},
	date = {2020-02-24},
	eprinttype = {arxiv},
	eprint = {1904.09675 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {Preprint PDF:/home/malde/Zotero/storage/3PLPUPA3/Zhang et al. - 2020 - BERTScore Evaluating Text Generation with BERT.pdf:application/pdf;Snapshot:/home/malde/Zotero/storage/7M39C2UG/1904.html:text/html},
}

@article{tarrant_frequency_2006,
	title = {The frequency of item writing flaws in multiple-choice questions used in high stakes nursing assessments},
	volume = {26},
	issn = {0260-6917},
	url = {https://www.sciencedirect.com/science/article/pii/S0260691706001067},
	doi = {10.1016/j.nedt.2006.07.006},
	series = {Proceedings from the 1st Nurse Education International Conference},
	abstract = {Multiple-choice questions are a common assessment method in nursing examinations. Few nurse educators, however, have formal preparation in constructing multiple-choice questions. Consequently, questions used in baccalaureate nursing assessments often contain item-writing flaws, or violations to accepted item-writing guidelines. In one nursing department, 2770 {MCQs} were collected from tests and examinations administered over a five-year period from 2001 to 2005. Questions were evaluated for 19 frequently occurring item-writing flaws, for cognitive level, for question source, and for the distribution of correct answers. Results show that almost half (46.2\%) of the questions contained violations of item-writing guidelines and over 90\% were written at low cognitive levels. Only a small proportion of questions were teacher generated (14.1\%), while 36.2\% were taken from testbanks and almost half (49.4\%) had no source identified. {MCQs} written at a lower cognitive level were significantly more likely to contain item-writing flaws. While there was no relationship between the source of the question and item-writing flaws, teachergenerated questions were more likely to be written at higher cognitive levels (p{\textless}0.001). Correct answers were evenly distributed across all four options and no bias was noted in the placement of correct options. Further training in item-writing is recommended for all faculty members who are responsible for developing tests. Pre-test review and quality assessment is also recommended to reduce the occurrence of item-writing flaws and to improve the quality of test questions.},
	pages = {662--671},
	number = {8},
	journaltitle = {Nurse Education Today},
	shortjournal = {Nurse Education Today},
	author = {Tarrant, Marie and Knierim, Aimee and Hayes, Sasha K. and Ware, James},
	urldate = {2025-04-29},
	date = {2006-12-01},
	keywords = {Multiple-choice questions, Assessment, Examination, Item-writing flaws},
	file = {PDF:/home/malde/Zotero/storage/QAVPY3W4/Tarrant et al. - 2006 - The frequency of item writing flaws in multiple-choice questions used in high stakes nursing assessm.pdf:application/pdf;ScienceDirect Snapshot:/home/malde/Zotero/storage/CH87ISRL/S0260691706001067.html:text/html},
}

@article{ji_qascoreunsupervised_2022,
	title = {{QAScore}—An Unsupervised Unreferenced Metric for the Question Generation Evaluation},
	volume = {24},
	rights = {http://creativecommons.org/licenses/by/3.0/},
	issn = {1099-4300},
	url = {https://www.mdpi.com/1099-4300/24/11/1514},
	doi = {10.3390/e24111514},
	abstract = {Question Generation ({QG}) aims to automate the task of composing questions for a passage with a set of chosen answers found within the passage. In recent years, the introduction of neural generation models has resulted in substantial improvements of automatically generated questions in terms of quality, especially compared to traditional approaches that employ manually crafted heuristics. However, current {QG} evaluation metrics solely rely on the comparison between the generated questions and references, ignoring the passages or answers. Meanwhile, these metrics are generally criticized because of their low agreement with human judgement. We therefore propose a new reference-free evaluation metric called {QAScore}, which is capable of providing a better mechanism for evaluating {QG} systems. {QAScore} evaluates a question by computing the cross entropy according to the probability that the language model can correctly generate the masked words in the answer to that question. Compared to existing metrics such as {BLEU} and {BERTScore}, {QAScore} can obtain a stronger correlation with human judgement according to our human evaluation experiment, meaning that applying {QAScore} in the {QG} task benefits to a higher level of evaluation accuracy.},
	pages = {1514},
	number = {11},
	journaltitle = {Entropy},
	author = {Ji, Tianbo and Lyu, Chenyang and Jones, Gareth and Zhou, Liting and Graham, Yvette},
	urldate = {2025-04-29},
	date = {2022-11},
	langid = {english},
	note = {Number: 11
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {question generation, question generation evaluation, reference-free evaluation},
	file = {Full Text PDF:/home/malde/Zotero/storage/FHX85KYC/Ji et al. - 2022 - QAScore—An Unsupervised Unreferenced Metric for the Question Generation Evaluation.pdf:application/pdf},
}

@misc{mohammadshahi_rquge_2023,
	title = {{RQUGE}: Reference-Free Metric for Evaluating Question Generation by Answering the Question},
	url = {http://arxiv.org/abs/2211.01482},
	doi = {10.48550/arXiv.2211.01482},
	shorttitle = {{RQUGE}},
	abstract = {Existing metrics for evaluating the quality of automatically generated questions such as {BLEU}, {ROUGE}, {BERTScore}, and {BLEURT} compare the reference and predicted questions, providing a high score when there is a considerable lexical overlap or semantic similarity between the candidate and the reference questions. This approach has two major shortcomings. First, we need expensive human-provided reference questions. Second, it penalises valid questions that may not have high lexical or semantic similarity to the reference questions. In this paper, we propose a new metric, {RQUGE}, based on the answerability of the candidate question given the context. The metric consists of a question-answering and a span scorer modules, using pre-trained models from existing literature, thus it can be used without any further training. We demonstrate that {RQUGE} has a higher correlation with human judgment without relying on the reference question. Additionally, {RQUGE} is shown to be more robust to several adversarial corruptions. Furthermore, we illustrate that we can significantly improve the performance of {QA} models on out-of-domain datasets by fine-tuning on synthetic data generated by a question generation model and re-ranked by {RQUGE}.},
	number = {{arXiv}:2211.01482},
	publisher = {{arXiv}},
	author = {Mohammadshahi, Alireza and Scialom, Thomas and Yazdani, Majid and Yanki, Pouya and Fan, Angela and Henderson, James and Saeidi, Marzieh},
	urldate = {2025-04-29},
	date = {2023-05-26},
	eprinttype = {arxiv},
	eprint = {2211.01482 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
	file = {Preprint PDF:/home/malde/Zotero/storage/CGY8RWJZ/Mohammadshahi et al. - 2023 - RQUGE Reference-Free Metric for Evaluating Question Generation by Answering the Question.pdf:application/pdf},
}

@misc{sellam_bleurt_2020,
	title = {{BLEURT}: Learning Robust Metrics for Text Generation},
	url = {http://arxiv.org/abs/2004.04696},
	doi = {10.48550/arXiv.2004.04696},
	shorttitle = {{BLEURT}},
	abstract = {Text generation has made significant advances in the last few years. Yet, evaluation metrics have lagged behind, as the most popular choices (e.g., {BLEU} and {ROUGE}) may correlate poorly with human judgments. We propose {BLEURT}, a learned evaluation metric based on {BERT} that can model human judgments with a few thousand possibly biased training examples. A key aspect of our approach is a novel pre-training scheme that uses millions of synthetic examples to help the model generalize. {BLEURT} provides state-of-the-art results on the last three years of the {WMT} Metrics shared task and the {WebNLG} Competition dataset. In contrast to a vanilla {BERT}-based approach, it yields superior results even when the training data is scarce and out-of-distribution.},
	number = {{arXiv}:2004.04696},
	publisher = {{arXiv}},
	author = {Sellam, Thibault and Das, Dipanjan and Parikh, Ankur P.},
	urldate = {2025-04-29},
	date = {2020-05-21},
	eprinttype = {arxiv},
	eprint = {2004.04696 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {Preprint PDF:/home/malde/Zotero/storage/UNK2IC7J/Sellam et al. - 2020 - BLEURT Learning Robust Metrics for Text Generation.pdf:application/pdf;Snapshot:/home/malde/Zotero/storage/AMA86FTR/2004.html:text/html},
}

@article{radford_language_2019,
	title = {Language models are unsupervised multitask learners},
	volume = {1},
	url = {https://storage.prod.researchhub.com/uploads/papers/2020/06/01/language-models.pdf},
	pages = {9},
	number = {8},
	journaltitle = {{OpenAI} blog},
	author = {Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya and {others}},
	urldate = {2025-05-09},
	date = {2019},
	file = {PDF:/home/malde/Zotero/storage/83L6NTJA/Radford et al. - Language Models are Unsupervised Multitask Learners.pdf:application/pdf},
}

@misc{glm_chatglm_2024,
	title = {{ChatGLM}: A Family of Large Language Models from {GLM}-130B to {GLM}-4 All Tools},
	url = {http://arxiv.org/abs/2406.12793},
	doi = {10.48550/arXiv.2406.12793},
	shorttitle = {{ChatGLM}},
	abstract = {We introduce {ChatGLM}, an evolving family of large language models that we have been developing over time. This report primarily focuses on the {GLM}-4 language series, which includes {GLM}-4, {GLM}-4-Air, and {GLM}-4-9B. They represent our most capable models that are trained with all the insights and lessons gained from the preceding three generations of {ChatGLM}. To date, the {GLM}-4 models are pre-trained on ten trillions of tokens mostly in Chinese and English, along with a small set of corpus from 24 languages, and aligned primarily for Chinese and English usage. The high-quality alignment is achieved via a multi-stage post-training process, which involves supervised fine-tuning and learning from human feedback. Evaluations show that {GLM}-4 1) closely rivals or outperforms {GPT}-4 in terms of general metrics such as {MMLU}, {GSM}8K, {MATH}, {BBH}, {GPQA}, and {HumanEval}, 2) gets close to {GPT}-4-Turbo in instruction following as measured by {IFEval}, 3) matches {GPT}-4 Turbo (128K) and Claude 3 for long context tasks, and 4) outperforms {GPT}-4 in Chinese alignments as measured by {AlignBench}. The {GLM}-4 All Tools model is further aligned to understand user intent and autonomously decide when and which tool(s) touse -- including web browser, Python interpreter, text-to-image model, and user-defined functions -- to effectively complete complex tasks. In practical applications, it matches and even surpasses {GPT}-4 All Tools in tasks like accessing online information via web browsing and solving math problems using Python interpreter. Over the course, we have open-sourced a series of models, including {ChatGLM}-6B (three generations), {GLM}-4-9B (128K, 1M), {GLM}-4V-9B, {WebGLM}, and {CodeGeeX}, attracting over 10 million downloads on Hugging face in the year 2023 alone. The open models can be accessed through https://github.com/{THUDM} and https://huggingface.co/{THUDM}.},
	number = {{arXiv}:2406.12793},
	publisher = {{arXiv}},
	author = {{GLM}, Team and Zeng, Aohan and Xu, Bin and Wang, Bowen and Zhang, Chenhui and Yin, Da and Zhang, Dan and Rojas, Diego and Feng, Guanyu and Zhao, Hanlin and Lai, Hanyu and Yu, Hao and Wang, Hongning and Sun, Jiadai and Zhang, Jiajie and Cheng, Jiale and Gui, Jiayi and Tang, Jie and Zhang, Jing and Sun, Jingyu and Li, Juanzi and Zhao, Lei and Wu, Lindong and Zhong, Lucen and Liu, Mingdao and Huang, Minlie and Zhang, Peng and Zheng, Qinkai and Lu, Rui and Duan, Shuaiqi and Zhang, Shudan and Cao, Shulin and Yang, Shuxun and Tam, Weng Lam and Zhao, Wenyi and Liu, Xiao and Xia, Xiao and Zhang, Xiaohan and Gu, Xiaotao and Lv, Xin and Liu, Xinghan and Liu, Xinyi and Yang, Xinyue and Song, Xixuan and Zhang, Xunkai and An, Yifan and Xu, Yifan and Niu, Yilin and Yang, Yuantao and Li, Yueyan and Bai, Yushi and Dong, Yuxiao and Qi, Zehan and Wang, Zhaoyu and Yang, Zhen and Du, Zhengxiao and Hou, Zhenyu and Wang, Zihan},
	urldate = {2025-05-09},
	date = {2024-07-30},
	eprinttype = {arxiv},
	eprint = {2406.12793 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {Preprint PDF:/home/malde/Zotero/storage/9QGR9ENB/GLM et al. - 2024 - ChatGLM A Family of Large Language Models from GLM-130B to GLM-4 All Tools.pdf:application/pdf;Snapshot:/home/malde/Zotero/storage/Z42ZAIAA/2406.html:text/html},
}

@article{madaan_self-refine_2023,
	title = {Self-Refine: Iterative Refinement with Self-Feedback},
	volume = {36},
	url = {https://proceedings.neurips.cc/paper_files/paper/2023/hash/91edff07232fb1b55a505a9e9f6c0ff3-Abstract-Conference.html},
	shorttitle = {Self-Refine},
	pages = {46534--46594},
	journaltitle = {Advances in Neural Information Processing Systems},
	author = {Madaan, Aman and Tandon, Niket and Gupta, Prakhar and Hallinan, Skyler and Gao, Luyu and Wiegreffe, Sarah and Alon, Uri and Dziri, Nouha and Prabhumoye, Shrimai and Yang, Yiming and Gupta, Shashank and Majumder, Bodhisattwa Prasad and Hermann, Katherine and Welleck, Sean and Yazdanbakhsh, Amir and Clark, Peter},
	urldate = {2025-05-09},
	date = {2023-12-15},
	langid = {english},
	file = {Full Text PDF:/home/malde/Zotero/storage/R9ZFM7AJ/Madaan et al. - 2023 - Self-Refine Iterative Refinement with Self-Feedback.pdf:application/pdf},
}

@article{wei_chain--thought_2022,
	title = {Chain-of-Thought Prompting Elicits Reasoning in Large Language Models},
	volume = {35},
	url = {https://proceedings.neurips.cc/paper_files/paper/2022/hash/9d5609613524ecf4f15af0f7b31abca4-Abstract-Conference.html?ref=https://githubhelp.com},
	pages = {24824--24837},
	journaltitle = {Advances in Neural Information Processing Systems},
	author = {Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Ichter, Brian and Xia, Fei and Chi, Ed and Le, Quoc V. and Zhou, Denny},
	urldate = {2025-05-09},
	date = {2022-12-06},
	langid = {english},
	file = {Full Text PDF:/home/malde/Zotero/storage/CJX38TMH/Wei et al. - 2022 - Chain-of-Thought Prompting Elicits Reasoning in Large Language Models.pdf:application/pdf},
}

@misc{ouyang_training_2022,
	title = {Training language models to follow instructions with human feedback},
	url = {http://arxiv.org/abs/2203.02155},
	doi = {10.48550/arXiv.2203.02155},
	abstract = {Making language models bigger does not inherently make them better at following a user’s intent. For example, large language models can generate outputs that are untruthful, toxic, or simply not helpful to the user. In other words, these models are not aligned with their users. In this paper, we show an avenue for aligning language models with user intent on a wide range of tasks by ﬁne-tuning with human feedback. Starting with a set of labeler-written prompts and prompts submitted through the {OpenAI} {API}, we collect a dataset of labeler demonstrations of the desired model behavior, which we use to ﬁne-tune {GPT}-3 using supervised learning. We then collect a dataset of rankings of model outputs, which we use to further ﬁne-tune this supervised model using reinforcement learning from human feedback. We call the resulting models {InstructGPT}. In human evaluations on our prompt distribution, outputs from the 1.3B parameter {InstructGPT} model are preferred to outputs from the 175B {GPT}-3, despite having 100x fewer parameters. Moreover, {InstructGPT} models show improvements in truthfulness and reductions in toxic output generation while having minimal performance regressions on public {NLP} datasets. Even though {InstructGPT} still makes simple mistakes, our results show that ﬁne-tuning with human feedback is a promising direction for aligning language models with human intent.},
	number = {{arXiv}:2203.02155},
	publisher = {{arXiv}},
	author = {Ouyang, Long and Wu, Jeff and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll L. and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and Schulman, John and Hilton, Jacob and Kelton, Fraser and Miller, Luke and Simens, Maddie and Askell, Amanda and Welinder, Peter and Christiano, Paul and Leike, Jan and Lowe, Ryan},
	urldate = {2025-05-09},
	date = {2022-03-04},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2203.02155 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
	file = {PDF:/home/malde/Zotero/storage/RYB6R2A3/Ouyang et al. - 2022 - Training language models to follow instructions with human feedback.pdf:application/pdf},
}

@article{cohen_coefficient_1960,
	title = {A Coefficient of Agreement for Nominal Scales},
	volume = {20},
	issn = {0013-1644},
	url = {https://doi.org/10.1177/001316446002000104},
	doi = {10.1177/001316446002000104},
	pages = {37--46},
	number = {1},
	journaltitle = {Educational and Psychological Measurement},
	author = {Cohen, Jacob},
	urldate = {2025-05-09},
	date = {1960-04-01},
	note = {Publisher: {SAGE} Publications Inc},
	file = {SAGE PDF Full Text:/home/malde/Zotero/storage/AJ5F2IXN/Cohen - 1960 - A Coefficient of Agreement for Nominal Scales.pdf:application/pdf},
}

@inproceedings{cavojsky_exploring_2023,
	title = {Exploring the Capabilities and Possible Applications of Large Language Models for Education},
	url = {https://ieeexplore.ieee.org/document/10344166},
	doi = {10.1109/ICETA61311.2023.10344166},
	abstract = {This research looks into the possible applications of large language models ({LLMs}) in education, with a special emphasis on {ChatGPT}, an {OpenAI}-developed chat bot model. It demonstrates the usefulness of {ChatGPT} as a learning aid and research assistant while addressing ethical and pedagogical concerns such as cognitive offloading, academic integrity, and critical thinking. The study covers a wide range of educational settings, including content development, learning a foreign language, problem solving, and literature review assistance. Furthermore, it presents {ZeroGPT}, a plagiarism detection tool for recognizing {GPT}-generated content and summarizes the findings to highlight {LLMs}’ ability to improve educational experiences.},
	eventtitle = {2023 21st International Conference on Emerging {eLearning} Technologies and Applications ({ICETA})},
	pages = {91--98},
	booktitle = {2023 21st International Conference on Emerging {eLearning} Technologies and Applications ({ICETA})},
	author = {Čavojský, Matúš and Bugár, Gabriel and Kormaník, Tomáš and Hasin, Martin},
	urldate = {2025-05-09},
	date = {2023-10},
	keywords = {{ChatGPT}, Ethics, Large Language Models, Education, Deep learning, Computer aided instruction, Educational Digital Transformation, Electronic learning, Feynman Technique, Integration of Educational Technology, Pandemics, Plagiarism, Prompt Engeneering},
	file = {Full Text PDF:/home/malde/Zotero/storage/2J8689HX/Čavojský et al. - 2023 - Exploring the Capabilities and Possible Applications of Large Language Models for Education.pdf:application/pdf},
}

@inproceedings{perez_automatic_2012,
	title = {Automatic classification of question difficulty level: Teachers' estimation vs. students' perception},
	url = {https://ieeexplore.ieee.org/abstract/document/6462398},
	doi = {10.1109/FIE.2012.6462398},
	shorttitle = {Automatic classification of question difficulty level},
	abstract = {The accurate estimation of the difficulty level of the questions posed to students is essential to help them to learn more effectively and efficiently. However, it is agreed that teachers usually fail to identify the correct difficulty level of the questions, according to the answers and final scores obtained by their students. Thus, this paper examines the ability of teachers for categorizing questions by difficulty level, comparing it with the students' perception and the measures obtained by an expert system of question automatic classification. The results show that students perceive questions more difficult than teachers, except for the harder ones. In addition, teachers are only lightly more accurate (closer to the expert system), in spite of the general students' tendency to overestimate the difficulty level of less difficult questions. Although no general conclusions can be obtained about behavior and accuracy of teachers and students when they analyze the difficulty of learning material, the provided analysis could be very valuable for teachers in order to detect unclear problem statements and students' misconceptions.},
	eventtitle = {2012 Frontiers in Education Conference},
	pages = {1--5},
	booktitle = {2012 Frontiers in Education Conference Proceedings},
	author = {Pérez, Elena Verdú and Santos, Luisa M. Regueras and Pérez, María Jesús Verdú and de Castro Fernández, Juan Pablo and Martín, Ricardo García},
	urldate = {2025-05-09},
	date = {2012-10},
	note = {{ISSN}: 2377-634X},
	keywords = {Accuracy, Electronic learning, automatic question classification, Context, Educational institutions, educational technology, estimation, Estimation, expert systems, Expert systems, Materials, perception, students', teachers'},
	file = {Full Text PDF:/home/malde/Zotero/storage/75SJW5BA/Pérez et al. - 2012 - Automatic classification of question difficulty level Teachers' estimation vs. students' perception.pdf:application/pdf},
}

@inproceedings{lewis_retrieval-augmented_2020,
	title = {Retrieval-Augmented Generation for Knowledge-Intensive {NLP} Tasks},
	volume = {33},
	url = {https://proceedings.neurips.cc/paper/2020/hash/6b493230205f780e1bc26945df7481e5-Abstract.html},
	abstract = {Large pre-trained language models have been shown to store factual knowledge in their parameters, and achieve state-of-the-art results when fine-tuned on downstream {NLP} tasks. However, their ability to access and precisely manipulate knowledge is still limited, and hence on knowledge-intensive tasks, their performance lags behind task-specific architectures. Additionally, providing provenance for their decisions and updating their world knowledge remain open research problems. Pre-trained models with a differentiable access mechanism to explicit non-parametric memory can overcome this issue, but have so far been only investigated for extractive downstream tasks. We explore a general-purpose fine-tuning recipe for retrieval-augmented generation ({RAG}) -- models which combine pre-trained parametric and non-parametric memory for language generation. We introduce {RAG} models where the parametric memory is a pre-trained seq2seq model and the non-parametric memory is a dense vector index of Wikipedia, accessed with a pre-trained neural retriever. We compare two {RAG} formulations, one which conditions on the same retrieved passages across the whole generated sequence, the other can use different passages per token. We fine-tune and evaluate our models on a wide range of knowledge-intensive {NLP} tasks and set the state-of-the-art on three open domain {QA} tasks, outperforming parametric seq2seq models and task-specific retrieve-and-extract architectures. For language generation tasks, we find that {RAG} models generate more specific, diverse and factual language than a state-of-the-art parametric-only seq2seq baseline.},
	pages = {9459--9474},
	booktitle = {Advances in Neural Information Processing Systems},
	publisher = {Curran Associates, Inc.},
	author = {Lewis, Patrick and Perez, Ethan and Piktus, Aleksandra and Petroni, Fabio and Karpukhin, Vladimir and Goyal, Naman and Küttler, Heinrich and Lewis, Mike and Yih, Wen-tau and Rocktäschel, Tim and Riedel, Sebastian and Kiela, Douwe},
	urldate = {2025-05-10},
	date = {2020},
	file = {Full Text PDF:/home/malde/Zotero/storage/W599FM2S/Lewis et al. - 2020 - Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks.pdf:application/pdf},
}
