
@article{shao_survey_2024,
	title = {Survey of Different Large Language Model Architectures: Trends, Benchmarks, and Challenges},
	volume = {12},
	issn = {2169-3536},
	url = {https://ieeexplore.ieee.org/document/10720163/?arnumber=10720163},
	doi = {10.1109/ACCESS.2024.3482107},
	shorttitle = {Survey of Different Large Language Model Architectures},
	abstract = {Large Language Models ({LLMs}) represent a class of deep learning models adept at understanding natural language and generating coherent responses to various prompts or queries. These models far exceed the complexity of conventional neural networks, often encompassing dozens of neural network layers and containing billions to trillions of parameters. They are typically trained on vast datasets, utilizing architectures based on transformer blocks. Present-day {LLMs} are multi-functional, capable of performing a range of tasks from text generation and language translation to question answering, as well as code generation and analysis. An advanced subset of these models, known as Multimodal Large Language Models ({MLLMs}), extends {LLM} capabilities to process and interpret multiple data modalities, including images, audio, and video. This enhancement empowers {MLLMs} with capabilities like video editing, image comprehension, and captioning for visual content. This survey provides a comprehensive overview of the recent advancements in {LLMs}. We begin by tracing the evolution of {LLMs} and subsequently delve into the advent and nuances of {MLLMs}. We analyze emerging state-of-the-art {MLLMs}, exploring their technical features, strengths, and limitations. Additionally, we present a comparative analysis of these models and discuss their challenges, potential limitations, and prospects for future development.},
	pages = {188664--188706},
	journaltitle = {{IEEE} Access},
	author = {Shao, Minghao and Basit, Abdul and Karri, Ramesh and Shafique, Muhammad},
	urldate = {2025-04-04},
	date = {2024},
	note = {Conference Name: {IEEE} Access},
	keywords = {Large language models, Transformers, Training, Adaptation models, Benchmark testing, Computational modeling, Decoding, deep learning, Encoding, generative models, Large language models ({LLMs}), Market research, multimodal learning, natural language processing ({NLP}), survey, Surveys, Transformer architecture, not really important, can be important},
	file = {Full Text PDF:/home/malde/Zotero/storage/6AGK56XY/Shao et al. - 2024 - Survey of Different Large Language Model Architectures Trends, Benchmarks, and Challenges.pdf:application/pdf;IEEE Xplore Abstract Record:/home/malde/Zotero/storage/9M5N96LQ/10720163.html:text/html},
}

@article{patil_review_2024,
	title = {A Review of Current Trends, Techniques, and Challenges in Large Language Models ({LLMs})},
	volume = {14},
	rights = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2076-3417},
	url = {https://www.mdpi.com/2076-3417/14/5/2074},
	doi = {10.3390/app14052074},
	abstract = {Natural language processing ({NLP}) has significantly transformed in the last decade, especially in the field of language modeling. Large language models ({LLMs}) have achieved {SOTA} performances on natural language understanding ({NLU}) and natural language generation ({NLG}) tasks by learning language representation in self-supervised ways. This paper provides a comprehensive survey to capture the progression of advances in language models. In this paper, we examine the different aspects of language models, which started with a few million parameters but have reached the size of a trillion in a very short time. We also look at how these {LLMs} transitioned from task-specific to task-independent to task-and-language-independent architectures. This paper extensively discusses different pretraining objectives, benchmarks, and transfer learning methods used in {LLMs}. It also examines different finetuning and in-context learning techniques used in downstream tasks. Moreover, it explores how {LLMs} can perform well across many domains and datasets if sufficiently trained on a large and diverse dataset. Next, it discusses how, over time, the availability of cheap computational power and large datasets have improved {LLM}’s capabilities and raised new challenges. As part of our study, we also inspect {LLMs} from the perspective of scalability to see how their performance is affected by the model’s depth, width, and data size. Lastly, we provide an empirical comparison of existing trends and techniques and a comprehensive analysis of where the field of {LLM} currently stands.},
	pages = {2074},
	number = {5},
	journaltitle = {Applied Sciences},
	author = {Patil, Rajvardhan and Gudivada, Venkat},
	urldate = {2025-04-04},
	date = {2024-01},
	langid = {english},
	note = {Number: 5
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {survey, language models, large language model, literature review, {LLMs}, natural language processing, {NLP}, {PLMs}, review, important},
	file = {Full Text PDF:/home/malde/Zotero/storage/2YASXKB8/Patil und Gudivada - 2024 - A Review of Current Trends, Techniques, and Challenges in Large Language Models (LLMs).pdf:application/pdf},
}

@article{fan_bibliometric_2024,
	title = {A Bibliometric Review of Large Language Models Research from 2017 to 2023},
	volume = {15},
	issn = {2157-6904, 2157-6912},
	url = {https://dl.acm.org/doi/10.1145/3664930},
	doi = {10.1145/3664930},
	abstract = {Large language models ({LLMs}), such as {OpenAI}'s Generative Pre-trained Transformer ({GPT}), are a class of language models that have demonstrated outstanding performance across a range of natural language processing ({NLP}) tasks. {LLMs} have become a highly sought-after research area because of their ability to generate human-like language and their potential to revolutionize science and technology. In this study, we conduct bibliometric and discourse analyses of scholarly literature on {LLMs}. Synthesizing over 5,000 publications, this article serves as a roadmap for researchers, practitioners, and policymakers to navigate the current landscape of {LLMs} research. We present the research trends from 2017 to early 2023, identifying patterns in research paradigms and collaborations. We start with analyzing the core algorithm developments and {NLP} tasks that are fundamental in {LLMs} research. We then investigate the applications of {LLMs} in various fields and domains, including medicine, engineering, social science, and humanities. Our review also reveals the dynamic, fast-paced evolution of {LLMs} research. Overall, this article offers valuable insights into the current state, impact, and potential of {LLMs} research and its applications.},
	pages = {1--25},
	number = {5},
	journaltitle = {{ACM} Transactions on Intelligent Systems and Technology},
	shortjournal = {{ACM} Trans. Intell. Syst. Technol.},
	author = {Fan, Lizhou and Li, Lingyao and Ma, Zihui and Lee, Sanggyu and Yu, Huizi and Hemphill, Libby},
	urldate = {2025-04-04},
	date = {2024-10-31},
	langid = {english},
	keywords = {not really important, can be important},
	file = {PDF:/home/malde/Zotero/storage/M7YEFTNM/Fan et al. - 2024 - A Bibliometric Review of Large Language Models Research from 2017 to 2023.pdf:application/pdf},
}

@misc{minaee_large_2025,
	title = {Large Language Models: A Survey},
	url = {http://arxiv.org/abs/2402.06196},
	doi = {10.48550/arXiv.2402.06196},
	shorttitle = {Large Language Models},
	abstract = {Large Language Models ({LLMs}) have drawn a lot of attention due to their strong performance on a wide range of natural language tasks, since the release of {ChatGPT} in November 2022. {LLMs}’ ability of general-purpose language understanding and generation is acquired by training billions of model’s parameters on massive amounts of text data, as predicted by scaling laws [1], [2]. The research area of {LLMs}, while very recent, is evolving rapidly in many different ways. In this paper, we review some of the most prominent {LLMs}, including three popular {LLM} families ({GPT}, {LLaMA}, {PaLM}), and discuss their characteristics, contributions and limitations. We also give an overview of techniques developed to build, and augment {LLMs}. We then survey popular datasets prepared for {LLM} training, fine-tuning, and evaluation, review widely used {LLM} evaluation metrics, and compare the performance of several popular {LLMs} on a set of representative benchmarks. Finally, we conclude the paper by discussing open challenges and future research directions.},
	number = {{arXiv}:2402.06196},
	publisher = {{arXiv}},
	author = {Minaee, Shervin and Mikolov, Tomas and Nikzad, Narjes and Chenaghlu, Meysam and Socher, Richard and Amatriain, Xavier and Gao, Jianfeng},
	urldate = {2025-04-04},
	date = {2025-03-23},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2402.06196 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence, can be important},
	file = {PDF:/home/malde/Zotero/storage/LLL62XQN/Minaee et al. - 2025 - Large Language Models A Survey.pdf:application/pdf},
}

@misc{liu_understanding_2024,
	title = {Understanding {LLMs}: A Comprehensive Overview from Training to Inference},
	url = {http://arxiv.org/abs/2401.02038},
	doi = {10.48550/arXiv.2401.02038},
	shorttitle = {Understanding {LLMs}},
	abstract = {The introduction of {ChatGPT} has led to a significant increase in the utilization of Large Language Models ({LLMs}) for addressing downstream tasks. There’s an increasing focus on cost-efficient training and deployment within this context. Low-cost training and deployment of {LLMs} represent the future development trend. This paper reviews the evolution of large language model training techniques and inference deployment technologies aligned with this emerging trend. The discussion on training includes various aspects, including data preprocessing, training architecture, pre-training tasks, parallel training, and relevant content related to model finetuning. On the inference side, the paper covers topics such as model compression, parallel computation, memory scheduling, and structural optimization. It also explores {LLMs}’ utilization and provides insights into their future development.},
	number = {{arXiv}:2401.02038},
	publisher = {{arXiv}},
	author = {Liu, Yiheng and He, Hao and Han, Tianle and Zhang, Xu and Liu, Mengyuan and Tian, Jiaming and Zhang, Yutong and Wang, Jiaqi and Gao, Xiaohui and Zhong, Tianyang and Pan, Yi and Xu, Shaochen and Wu, Zihao and Liu, Zhengliang and Zhang, Xin and Zhang, Shu and Hu, Xintao and Zhang, Tuo and Qiang, Ning and Liu, Tianming and Ge, Bao},
	urldate = {2025-04-04},
	date = {2024-01-06},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2401.02038 [cs]},
	keywords = {Computer Science - Computation and Language, can be important},
	file = {Final:/home/malde/Zotero/storage/VI5CNRH9/1-s2.0-S0925231224019611-main.pdf:application/pdf;Preprint:/home/malde/Zotero/storage/6PNGXQTQ/Liu et al. - 2024 - Understanding LLMs A Comprehensive Overview from Training to Inference.pdf:application/pdf},
}

@inproceedings{wang_towards_2022,
	location = {Cham},
	title = {Towards Human-Like Educational Question Generation with Large Language Models},
	isbn = {978-3-031-11644-5},
	doi = {10.1007/978-3-031-11644-5_13},
	abstract = {We investigate the utility of large pretrained language models ({PLMs}) for automatic educational assessment question generation. While {PLMs} have shown increasing promise in a wide range of natural language applications, including question generation, they can generate unreliable and undesirable content. For high-stakes applications such as educational assessments, it is not only critical to ensure that the generated content is of high quality but also relates to the specific content being assessed. In this paper, we investigate the impact of various {PLM} prompting strategies on the quality of generated questions. We design a series of generation scenarios to evaluate various generation strategies and evaluate generated questions via automatic metrics and manual examination. With empirical evaluation, we identify the prompting strategy that is most likely to lead to high-quality generated questions. Finally, we demonstrate the promising educational utility of generated questions using our concluded best generation strategy by presenting generated questions together with human-authored questions to a subject matter expert, who despite their expertise, could not effectively distinguish between generated and human-authored questions.},
	pages = {153--166},
	booktitle = {Artificial Intelligence  in Education},
	publisher = {Springer International Publishing},
	author = {Wang, Zichao and Valdez, Jakob and Basu Mallick, Debshila and Baraniuk, Richard G.},
	editor = {Rodrigo, Maria Mercedes and Matsuda, Noburu and Cristea, Alexandra I. and Dimitrova, Vania},
	urldate = {2025-04-23},
	date = {2022},
	langid = {english},
	keywords = {not really important},
	file = {Full Text PDF:/home/malde/Zotero/storage/8GVTH8PQ/Wang et al. - 2022 - Towards Human-Like Educational Question Generation with Large Language Models.pdf:application/pdf},
}

@misc{nguyen_reference-based_2024,
	title = {Reference-based Metrics Disprove Themselves in Question Generation},
	url = {http://arxiv.org/abs/2403.12242},
	doi = {10.48550/arXiv.2403.12242},
	abstract = {Reference-based metrics such as {BLEU} and {BERTScore} are widely used to evaluate question generation ({QG}). In this study, on {QG} benchmarks such as {SQuAD} and {HotpotQA}, we find that using human-written references cannot guarantee the effectiveness of the reference-based metrics. Most {QG} benchmarks have only one reference; we replicate the annotation process and collect another reference. A good metric is expected to grade a human-validated question no worse than generated questions. However, the results of reference-based metrics on our newly collected reference disproved the metrics themselves. We propose a reference-free metric consisted of multi-dimensional criteria such as naturalness, answerability, and complexity, utilizing large language models. These criteria are not constrained to the syntactic or semantic of a single reference question, and the metric does not require a diverse set of references. Experiments reveal that our metric accurately distinguishes between high-quality questions and flawed ones, and achieves state-of-the-art alignment with human judgment.},
	number = {{arXiv}:2403.12242},
	publisher = {{arXiv}},
	author = {Nguyen, Bang and Yu, Mengxia and Huang, Yun and Jiang, Meng},
	urldate = {2025-04-04},
	date = {2024-10-10},
	eprinttype = {arxiv},
	eprint = {2403.12242 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence, Computer Science - Machine Learning, can be important},
	file = {Preprint PDF:/home/malde/Zotero/storage/JN6CUUQY/Nguyen et al. - 2024 - Reference-based Metrics Disprove Themselves in Question Generation.pdf:application/pdf;Snapshot:/home/malde/Zotero/storage/QYP38CKH/2403.html:text/html},
}

@inproceedings{mi_comparative_2024,
	title = {A Comparative Analysis of Different Large Language Models in Evaluating Student-Generated Questions},
	url = {https://ieeexplore.ieee.org/document/10540914},
	doi = {10.1109/ICEIT61397.2024.10540914},
	abstract = {Student-generated questions ({SGQs}) have proven to be a meaningful learning tool, fostering advanced thinking skills in students and aiding teachers in understanding student learning progress. However, grading the quality of {SGQs} demands significant effort from teachers. In this study, we explore the suitability of large language models in evaluating {SGQs} and identify which models can effectively replace expert evaluation of practical teaching problems. We devised a five-dimension scale, using expert ratings as the gold standard, and employed Kendall's W consistency analysis to systematically compare different large language model evaluations against expert ratings from six aspects of the scale. The research confirmed the applicability of large language models ({LLMs}) for the evaluation of {SGQs} and the exceptional performance of {ChatGPT} 4.0, which can assist experts in evaluating {SGQs}. This study aims to facilitate the implementation of artificial intelligence generated content ({AIGC}) in education and reinforces the belief in the substantial potential of large language models for future applications and research in the field of education.},
	eventtitle = {2024 13th International Conference on Educational and Information Technology ({ICEIT})},
	pages = {24--29},
	booktitle = {2024 13th International Conference on Educational and Information Technology ({ICEIT})},
	author = {Mi, Zejia and Li, Kangkang},
	urldate = {2025-04-04},
	date = {2024-03},
	keywords = {{ChatGPT}, large language model, Education, artificial intelligence, Chatbots, Data analysis, evaluation, Information technology, Robustness, Stability analysis, student-generated question, can be important},
	file = {Full Text PDF:/home/malde/Zotero/storage/BRDPABET/Mi und Li - 2024 - A Comparative Analysis of Different Large Language Models in Evaluating Student-Generated Questions.pdf:application/pdf;IEEE Xplore Abstract Record:/home/malde/Zotero/storage/UT2SQBDI/10540914.html:text/html},
}

@article{steuer_i_2021,
	title = {I Do Not Understand What I Cannot Define: Automatic Question Generation With Pedagogically-Driven Content Selection},
	url = {https://www.semanticscholar.org/paper/I-Do-Not-Understand-What-I-Cannot-Define%3A-Automatic-Steuer-Filighera/a0fd69c2540158fe79ff8cdba3d36a71e737211a},
	shorttitle = {I Do Not Understand What I Cannot Define},
	abstract = {Most learners fail to develop deep text comprehension when reading textbooks passively. Posing questions about what learners have read is a well-established way of fostering their text comprehension. However, many textbooks lack self-assessment questions because authoring them is timeconsuming and expensive. Automatic question generators may alleviate this scarcity by generating sound pedagogical questions. However, generating questions automatically poses linguistic and pedagogical challenges. What should we ask? And, how do we phrase the question automatically? We address those challenges with an automatic question generator grounded in learning theory. The paper introduces a novel pedagogically meaningful content selection mechanism to find question-worthy sentences and answers in arbitrary textbook contents. We conducted an empirical evaluation study with educational experts, annotating 150 generated questions in six different domains. Results indicate a high linguistic quality of the generated questions. Furthermore, the evaluation results imply that the majority of the generated questions inquire central information related to the given text and may foster text comprehension in specific learning scenarios.},
	journaltitle = {{ArXiv}},
	author = {Steuer, Tim and Filighera, Anna and Meuser, Tobias and Rensing, Christoph},
	urldate = {2025-04-04},
	date = {2021-10-08},
	keywords = {not really important, limitation/loser},
	file = {Full Text PDF:/home/malde/Zotero/storage/KNWEYR5Y/Steuer et al. - 2021 - I Do Not Understand What I Cannot Define Automatic Question Generation With Pedagogically-Driven Co.pdf:application/pdf},
}

@inproceedings{moore_assessing_2022,
	location = {Cham},
	title = {Assessing the Quality of Student-Generated Short Answer Questions Using {GPT}-3},
	isbn = {978-3-031-16290-9},
	doi = {10.1007/978-3-031-16290-9_18},
	abstract = {Generating short answer questions is a popular form of learnersourcing with benefits for both the students’ higher-order thinking and the instructors’ collection of assessment items. However, assessing the quality of the student-generated questions can involve significant efforts from instructors and domain experts. In this work, we investigate the feasibility of leveraging students to generate short answer questions with minimal scaffolding and machine learning models to evaluate the student-generated questions. We had 143 students across 7 online college-level chemistry courses participate in an activity where they were prompted to generate a short answer question regarding the content they were presently learning. Using both human and automatic evaluation methods, we investigated the linguistic and pedagogical quality of these student-generated questions. Our results showed that 32\% of the student-generated questions were evaluated by experts as high quality, indicating that they could be added and used in the course in their present condition. Additional expert evaluation identified that 23\% of the student-generated questions assessed higher cognitive processes according to Bloom’s Taxonomy. We also identified the strengths and weaknesses of using a state-of-the-art language model, {GPT}-3, to automatically evaluate the student-generated questions. Our findings suggest that students are relatively capable of generating short answer questions that can be leveraged in their online courses. Based on the evaluation methods, recommendations for leveraging experts and automatic methods are discussed.},
	pages = {243--257},
	booktitle = {Educating for a New Future: Making Sense of Technology-Enhanced Learning Adoption},
	publisher = {Springer International Publishing},
	author = {Moore, Steven and Nguyen, Huy A. and Bier, Norman and Domadia, Tanvi and Stamper, John},
	editor = {Hilliger, Isabel and Muñoz-Merino, Pedro J. and De Laet, Tinne and Ortega-Arranz, Alejandro and Farrell, Tracie},
	urldate = {2025-04-23},
	date = {2022},
	langid = {english},
	keywords = {Question evaluation, Question generation, Question quality, limitation/loser},
	file = {Full Text PDF:/home/malde/Zotero/storage/A48UTNWN/Moore et al. - 2022 - Assessing the Quality of Student-Generated Short Answer Questions Using GPT-3.pdf:application/pdf},
}

@article{raiaan_review_2024,
	title = {A Review on Large Language Models: Architectures, Applications, Taxonomies, Open Issues and Challenges},
	volume = {12},
	issn = {2169-3536},
	url = {https://ieeexplore.ieee.org/document/10433480},
	doi = {10.1109/ACCESS.2024.3365742},
	shorttitle = {A Review on Large Language Models},
	abstract = {Large Language Models ({LLMs}) recently demonstrated extraordinary capability in various natural language processing ({NLP}) tasks including language translation, text generation, question answering, etc. Moreover, {LLMs} are new and essential part of computerized language processing, having the ability to understand complex verbal patterns and generate coherent and appropriate replies in a given context. Though this success of {LLMs} has prompted a substantial increase in research contributions, rapid growth has made it difficult to understand the overall impact of these improvements. Since a plethora of research on {LLMs} have been appeared within a short time, it is quite impossible to track all of these and get an overview of the current state of research in this area. Consequently, the research community would benefit from a short but thorough review of the recent changes in this area. This article thoroughly overviews {LLMs}, including their history, architectures, transformers, resources, training methods, applications, impacts, challenges, etc. This paper begins by discussing the fundamental concepts of {LLMs} with its traditional pipeline of the {LLMs} training phase. Then the paper provides an overview of the existing works, the history of {LLMs}, their evolution over time, the architecture of transformers in {LLMs}, the different resources of {LLMs}, and the different training methods that have been used to train them. The paper also demonstrates the datasets utilized in the studies. After that, the paper discusses the wide range of applications of {LLMs}, including biomedical and healthcare, education, social, business, and agriculture. The study also illustrates how {LLMs} create an impact on society and shape the future of {AI} and how they can be used to solve real-world problems. Finally, the paper also explores open issues and challenges to deploy {LLMs} in real-world scenario. Our review paper aims to help practitioners, researchers, and experts thoroughly understand the evolution of {LLMs}, pre-trained architectures, applications, challenges, and future goals.},
	pages = {26839--26874},
	journaltitle = {{IEEE} Access},
	author = {Raiaan, Mohaimenul Azam Khan and Mukta, Md. Saddam Hossain and Fatema, Kaniz and Fahad, Nur Mohammad and Sakib, Sadman and Mim, Most Marufatul Jannat and Ahmad, Jubaer and Ali, Mohammed Eunus and Azam, Sami},
	urldate = {2025-04-06},
	date = {2024},
	keywords = {Transformers, Natural language processing, Training, natural language processing ({NLP}), Surveys, Taxonomy, artificial intelligence, application, Artificial intelligence, Cognition, Information analysis, Large language models ({LLM}), Linguistics, pre-trained models, Question answering (information retrieval), Task analysis, taxonomy, transformer, can be important},
	file = {Full Text PDF:/home/malde/Zotero/storage/C558I62B/Raiaan et al. - 2024 - A Review on Large Language Models Architectures, Applications, Taxonomies, Open Issues and Challeng.pdf:application/pdf},
}

@misc{naveed_comprehensive_2024,
	title = {A Comprehensive Overview of Large Language Models},
	url = {http://arxiv.org/abs/2307.06435},
	doi = {10.48550/arXiv.2307.06435},
	abstract = {Large Language Models ({LLMs}) have recently demonstrated remarkable capabilities in natural language processing tasks and beyond. This success of {LLMs} has led to a large influx of research contributions in this direction. These works encompass diverse topics such as architectural innovations, better training strategies, context length improvements, fine-tuning, multi-modal {LLMs}, robotics, datasets, benchmarking, efficiency, and more. With the rapid development of techniques and regular breakthroughs in {LLM} research, it has become considerably challenging to perceive the bigger picture of the advances in this direction. Considering the rapidly emerging plethora of literature on {LLMs}, it is imperative that the research community is able to benefit from a concise yet comprehensive overview of the recent developments in this field. This article provides an overview of the existing literature on a broad range of {LLM}-related concepts. Our self-contained comprehensive overview of {LLMs} discusses relevant background concepts along with covering the advanced topics at the frontier of research in {LLMs}. This review article is intended to not only provide a systematic survey but also a quick comprehensive reference for the researchers and practitioners to draw insights from extensive informative summaries of the existing works to advance the {LLM} research.},
	number = {{arXiv}:2307.06435},
	publisher = {{arXiv}},
	author = {Naveed, Humza and Khan, Asad Ullah and Qiu, Shi and Saqib, Muhammad and Anwar, Saeed and Usman, Muhammad and Akhtar, Naveed and Barnes, Nick and Mian, Ajmal},
	urldate = {2025-04-06},
	date = {2024-10-17},
	eprinttype = {arxiv},
	eprint = {2307.06435 [cs]},
	keywords = {Computer Science - Computation and Language, important},
	file = {Preprint PDF:/home/malde/Zotero/storage/2W5F2DH7/Naveed et al. - 2024 - A Comprehensive Overview of Large Language Models.pdf:application/pdf;Snapshot:/home/malde/Zotero/storage/7PLWZXBU/2307.html:text/html},
}

@misc{zhao_survey_2025,
	title = {A Survey of Large Language Models},
	url = {http://arxiv.org/abs/2303.18223},
	doi = {10.48550/arXiv.2303.18223},
	abstract = {Ever since the Turing Test was proposed in the 1950s, humans have explored the mastering of language intelligence by machine. Language is essentially a complex, intricate system of human expressions governed by grammatical rules. It poses a significant challenge to develop capable artificial intelligence ({AI}) algorithms for comprehending and grasping a language. As a major approach, language modeling has been widely studied for language understanding and generation in the past two decades, evolving from statistical language models to neural language models. Recently, pre-trained language models ({PLMs}) have been proposed by pretraining Transformer models over large-scale corpora, showing strong capabilities in solving various natural language processing ({NLP}) tasks. Since the researchers have found that model scaling can lead to an improved model capacity, they further investigate the scaling effect by increasing the parameter scale to an even larger size. Interestingly, when the parameter scale exceeds a certain level, these enlarged language models not only achieve a significant performance improvement, but also exhibit some special abilities (e.g., incontext learning) that are not present in small-scale language models (e.g., {BERT}). To discriminate the language models in different parameter scales, the research community has coined the term large language models ({LLM}) for the {PLMs} of significant size (e.g., containing tens or hundreds of billions of parameters). Recently, the research on {LLMs} has been largely advanced by both academia and industry, and a remarkable progress is the launch of {ChatGPT} (a powerful {AI} chatbot developed based on {LLMs}), which has attracted widespread attention from society. The technical evolution of {LLMs} has been making an important impact on the entire {AI} community, which would revolutionize the way how we develop and use {AI} algorithms. Considering this rapid technical progress, in this survey, we review the recent advances of {LLMs} by introducing the background, key findings, and mainstream techniques. In particular, we focus on four major aspects of {LLMs}, namely pre-training, adaptation tuning, utilization, and capacity evaluation. Furthermore, we also summarize the available resources for developing {LLMs} and discuss the remaining issues for future directions. This survey provides an up-to-date review of the literature on {LLMs}, which can be a useful resource for both researchers and engineers.},
	number = {{arXiv}:2303.18223},
	publisher = {{arXiv}},
	author = {Zhao, Wayne Xin and Zhou, Kun and Li, Junyi and Tang, Tianyi and Wang, Xiaolei and Hou, Yupeng and Min, Yingqian and Zhang, Beichen and Zhang, Junjie and Dong, Zican and Du, Yifan and Yang, Chen and Chen, Yushuo and Chen, Zhipeng and Jiang, Jinhao and Ren, Ruiyang and Li, Yifan and Tang, Xinyu and Liu, Zikang and Liu, Peiyu and Nie, Jian-Yun and Wen, Ji-Rong},
	urldate = {2025-04-06},
	date = {2025-03-11},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2303.18223 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence, important},
	file = {PDF:/home/malde/Zotero/storage/MHEI3P65/Zhao et al. - 2025 - A Survey of Large Language Models.pdf:application/pdf},
}

@inproceedings{naik_large_2024,
	location = {Cham},
	title = {Large Data Begets Large Data: Studying Large Language Models ({LLMs}) and Its History, Types, Working, Benefits and Limitations},
	isbn = {978-3-031-74443-3},
	doi = {10.1007/978-3-031-74443-3_18},
	shorttitle = {Large Data Begets Large Data},
	abstract = {The emergence of Large Language Models ({LLMs}) transformed the domain of Natural Language Processing ({NLP}) by enhancing the capability of machines to effectively comprehend and generate natural language. These {LLMs} are a type of generative {AI} and the underlying {AI} model that work behind the scenes of most modern {AI} chatbots. Generative {AI} is an umbrella term for all {AI} technologies which can generate original contents. These {LLMs} are pre-trained on a large amount of textual data and billions of parameters. This pre-training is normally unsupervised learning, meaning that it processes the unlabelled data for a comprehensive understanding of context, semantics, and grammar of natural language to generate coherent, context-relevant and credible text. In view of the significance of {LLMs}, this paper aims to perform a comprehensive study of {LLMs}, which will elucidate the historical journey of language processing and modelling, evolution and types of language models, and working, benefits and limitations of {LLMs}.},
	pages = {293--314},
	booktitle = {Contributions Presented at The International Conference on Computing, Communication, Cybersecurity and {AI}, July 3–4, 2024, London, {UK}},
	publisher = {Springer Nature Switzerland},
	author = {Naik, Dishita and Naik, Ishita and Naik, Nitin},
	editor = {Naik, Nitin and Jenkins, Paul and Prajapat, Shaligram and Grace, Paul},
	urldate = {2025-04-23},
	date = {2024},
	langid = {english},
	keywords = {not really important, can be important},
	file = {PDF:/home/malde/Zotero/storage/ALI34PSV/Naik et al. - 2024 - Large Data Begets Large Data Studying Large Language Models (LLMs) and Its History, Types, Working,.pdf:application/pdf},
}

@inproceedings{vaswani_attention_2017,
	title = {Attention is All you Need},
	volume = {30},
	url = {https://proceedings.neurips.cc/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html},
	abstract = {The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 {BLEU} {onEnglish}-to-German translation, improving over the existing best ensemble result by over 1 {BLEU}. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 {BLEU}, achieving a {BLEU} score of 41.1.},
	booktitle = {Advances in Neural Information Processing Systems},
	publisher = {Curran Associates, Inc.},
	author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, Ł ukasz and Polosukhin, Illia},
	urldate = {2025-04-06},
	date = {2017},
	keywords = {important},
	file = {Full Text PDF:/home/malde/Zotero/storage/9P2T5HWJ/Vaswani et al. - 2017 - Attention is All you Need.pdf:application/pdf},
}

@article{hou_large_2024,
	title = {Large Language Models for Software Engineering: A Systematic Literature Review},
	volume = {33},
	issn = {1049-331X},
	url = {https://dl.acm.org/doi/10.1145/3695988},
	doi = {10.1145/3695988},
	shorttitle = {Large Language Models for Software Engineering},
	abstract = {Large Language Models ({LLMs}) have significantly impacted numerous domains, including Software Engineering ({SE}). Many recent publications have explored {LLMs} applied to various {SE} tasks. Nevertheless, a comprehensive understanding of the application, effects, and possible limitations of {LLMs} on {SE} is still in its early stages. To bridge this gap, we conducted a Systematic Literature Review ({SLR}) on {LLM}4SE, with a particular focus on understanding how {LLMs} can be exploited to optimize processes and outcomes. We selected and analyzed 395 research articles from January 2017 to January 2024 to answer four key Research Questions ({RQs}). In {RQ}1, we categorize different {LLMs} that have been employed in {SE} tasks, characterizing their distinctive features and uses. In {RQ}2, we analyze the methods used in data collection, pre-processing, and application, highlighting the role of well-curated datasets for successful {LLM} for {SE} implementation. {RQ}3 investigates the strategies employed to optimize and evaluate the performance of {LLMs} in {SE}. Finally, {RQ}4 examines the specific {SE} tasks where {LLMs} have shown success to date, illustrating their practical contributions to the field. From the answers to these {RQs}, we discuss the current state-of-the-art and trends, identifying gaps in existing research, and highlighting promising areas for future study. Our artifacts are publicly available at .},
	pages = {220:1--220:79},
	number = {8},
	journaltitle = {{ACM} Trans. Softw. Eng. Methodol.},
	author = {Hou, Xinyi and Zhao, Yanjie and Liu, Yue and Yang, Zhou and Wang, Kailong and Li, Li and Luo, Xiapu and Lo, David and Grundy, John and Wang, Haoyu},
	urldate = {2025-04-06},
	date = {2024-12-03},
	keywords = {can be important},
	file = {Full Text PDF:/home/malde/Zotero/storage/N95I45FM/Hou et al. - 2024 - Large Language Models for Software Engineering A Systematic Literature Review.pdf:application/pdf},
}

@article{wang_history_2024,
	title = {History, development, and principles of large language models: an introductory survey},
	issn = {2730-5961},
	url = {https://doi.org/10.1007/s43681-024-00583-7},
	doi = {10.1007/s43681-024-00583-7},
	shorttitle = {History, development, and principles of large language models},
	abstract = {Language models serve as a cornerstone in natural language processing, utilizing mathematical methods to generalize language laws and knowledge for prediction and generation. Over extensive research spanning decades, language modeling has progressed from initial statistical language models to the contemporary landscape of large language models ({LLMs}). Notably, the swift evolution of {LLMs} has reached the ability to process, understand, and generate human-level text. Nevertheless, despite the significant advantages that {LLMs} offer in improving both work and personal lives, the limited understanding among general practitioners about the background and principles of these models hampers their full potential. Notably, most {LLM} reviews focus on specific aspects and utilize specialized language, posing a challenge for practitioners lacking relevant background knowledge. In light of this, this survey aims to present a comprehensible overview of {LLMs} to assist a broader audience. It strives to facilitate a comprehensive understanding by exploring the historical background of language models and tracing their evolution over time. The survey further investigates the factors influencing the development of {LLMs}, emphasizing key contributions. Additionally, it concentrates on elucidating the underlying principles of {LLMs}, equipping audiences with essential theoretical knowledge. The survey also highlights the limitations of existing work and points out promising future directions.},
	journaltitle = {{AI} and Ethics},
	shortjournal = {{AI} Ethics},
	author = {Wang, Zichong and Chu, Zhibo and Doan, Thang Viet and Ni, Shiwen and Yang, Min and Zhang, Wenbin},
	urldate = {2025-04-06},
	date = {2024-10-14},
	langid = {english},
	keywords = {Artificial Intelligence, Natural language processing, Artificial intelligence, can be important, Language model, Large language model},
	file = {Full Text PDF:/home/malde/Zotero/storage/5QJMHZSU/Wang et al. - 2024 - History, development, and principles of large language models an introductory survey.pdf:application/pdf},
}

@report{noauthor_llm_nodate,
	title = {{LLM} Leaderboard},
	url = {https://www.vellum.ai/llm-leaderboard},
	keywords = {important},
}

@inproceedings{scaria_automated_2024,
	location = {Cham},
	title = {Automated Educational Question Generation at Different Bloom’s Skill Levels Using Large Language Models: Strategies and Evaluation},
	isbn = {978-3-031-64299-9},
	doi = {10.1007/978-3-031-64299-9_12},
	shorttitle = {Automated Educational Question Generation at Different Bloom’s Skill Levels Using Large Language Models},
	abstract = {Developing questions that are pedagogically sound, relevant, and promote learning is a challenging and time-consuming task for educators. Modern-day large language models ({LLMs}) generate high-quality content across multiple domains, potentially helping educators to develop high-quality questions. Automated educational question generation ({AEQG}) is important in scaling online education catering to a diverse student population. Past attempts at {AEQG} have shown limited abilities to generate questions at higher cognitive levels. In this study, we examine the ability of five state-of-the-art {LLMs} of different sizes to generate diverse and high-quality questions of different cognitive levels, as defined by Bloom’s taxonomy. We use advanced prompting techniques with varying complexity for {AEQG}. We conducted expert and {LLM}-based evaluations to assess the linguistic and pedagogical relevance and quality of the questions. Our findings suggest that {LLMs} can generate relevant and high-quality educational questions of different cognitive levels when prompted with adequate information, although there is a significant variance in the performance of the five {LLMs} considered. We also show that automated evaluation is not on par with human evaluation.},
	pages = {165--179},
	booktitle = {Artificial Intelligence in Education},
	publisher = {Springer Nature Switzerland},
	author = {Scaria, Nicy and Dharani Chenna, Suma and Subramani, Deepak},
	editor = {Olney, Andrew M. and Chounta, Irene-Angelica and Liu, Zitao and Santos, Olga C. and Bittencourt, Ig Ibert},
	urldate = {2025-04-07},
	date = {2024},
	langid = {english},
	keywords = {Large Language Models, Automated Educational Question Generation, Bloom’s Taxonomy, limitation/loser},
	file = {Full Text PDF:/home/malde/Zotero/storage/WHFQRRF2/Scaria et al. - 2024 - Automated Educational Question Generation at Different Bloom’s Skill Levels Using Large Language Mod.pdf:application/pdf},
}

@article{maity_can_2025,
	title = {Can large language models meet the challenge of generating school-level questions?},
	volume = {8},
	issn = {2666-920X},
	url = {https://www.sciencedirect.com/science/article/pii/S2666920X25000104},
	doi = {10.1016/j.caeai.2025.100370},
	abstract = {In the realm of education, crafting appropriate questions for examinations is a meticulous and time-consuming task that is crucial for assessing students' understanding of the subject matter. This paper explores the potential of leveraging large language models ({LLMs}) to automate question generation in the educational domain. Specifically, we focus on generating educational questions from contexts extracted from school-level textbooks. Our study aims to prompt {LLMs} such as {GPT}-4 Turbo, {GPT}-3.5 Turbo, Llama-2-70B, Llama-3.1-405B, and Gemini Pro to generate a complete set of questions for each context, potentially streamlining the question generation process for educators. We performed a human evaluation of the generated questions, assessing their coverage, grammaticality, usefulness, answerability, and relevance. Additionally, we prompted {LLMs} to generate questions based on Bloom's revised taxonomy, categorizing and evaluating these questions according to their cognitive complexity and learning objectives. We applied both zero-shot and eight-shot prompting techniques. These efforts provide insight into the efficacy of {LLMs} in automated question generation and their potential in assessing students' cognitive abilities across various school-level subjects. The results show that employing an eight-shot technique improves the performance of human evaluation metrics for the generated complete set of questions and helps generate questions that are better aligned with Bloom's revised taxonomy.},
	pages = {100370},
	journaltitle = {Computers and Education: Artificial Intelligence},
	shortjournal = {Computers and Education: Artificial Intelligence},
	author = {Maity, Subhankar and Deroy, Aniket and Sarkar, Sudeshna},
	urldate = {2025-04-07},
	date = {2025-06-01},
	keywords = {Large language models ({LLMs}), can be important, Automated question generation ({AQG}), Bloom's revised taxonomy, {GPT}, Prompt},
	file = {PDF:/home/malde/Zotero/storage/YTG2D5N9/Maity et al. - 2025 - Can large language models meet the challenge of generating school-level questions.pdf:application/pdf;ScienceDirect Snapshot:/home/malde/Zotero/storage/JKCFYD5A/S2666920X25000104.html:text/html},
}

@inproceedings{bhowmick_automating_2023,
	location = {Cham},
	title = {Automating Question Generation From Educational Text},
	isbn = {978-3-031-47994-6},
	doi = {10.1007/978-3-031-47994-6_38},
	abstract = {The use of question-based activities ({QBAs}) is wide-spread in education, traditionally forming an integral part of the learning and assessment process. In this paper, we design and evaluate an automated question generation tool for formative and summative assessment in schools. We present an expert survey of one hundred and four teachers, demonstrating the need for automated generation of {QBAs}, as a tool that can significantly reduce the workload of teachers and facilitate personalized learning experiences. Leveraging the recent advancements in generative {AI}, we then present a modular framework employing transformer based language models for automatic generation of multiple-choice questions ({MCQs}) from textual content. The presented solution, with distinct modules for question generation, correct answer prediction, and distractor formulation, enables us to evaluate different language models and generation techniques. Finally, we perform an extensive quantitative and qualitative evaluation, demonstrating trade-offs in the use of different techniques and models.},
	pages = {437--450},
	booktitle = {Artificial Intelligence {XL}},
	publisher = {Springer Nature Switzerland},
	author = {Bhowmick, Ayan Kumar and Jagmohan, Ashish and Vempaty, Aditya and Dey, Prasenjit and Hall, Leigh and Hartman, Jeremy and Kokku, Ravi and Maheshwari, Hema},
	editor = {Bramer, Max and Stahl, Frederic},
	urldate = {2025-04-10},
	date = {2023},
	langid = {english},
	keywords = {Large language models, can be important, Automatic question generation, Distractors, Generative {AI}, Multiple-choice questions, Question-based activities},
	file = {Full Text PDF:/home/malde/Zotero/storage/R3WW43WB/Bhowmick et al. - 2023 - Automating Question Generation From Educational Text.pdf:application/pdf},
}

@article{ling_automatic_2024,
	title = {Automatic question-answer pairs generation using pre-trained large language models in higher education},
	volume = {6},
	issn = {2666-920X},
	url = {https://www.sciencedirect.com/science/article/pii/S2666920X24000559},
	doi = {10.1016/j.caeai.2024.100252},
	abstract = {The process of manually generating question and answer ({QA}) pairs for assessments is known to be a time-consuming and energy-intensive task for teachers, specifically in higher education. Several studies have proposed various methods utilising pre-trained large language models for the generation of {QA} pairs. However, it is worth noting that these methods have primarily been evaluated on datasets that are not specifically educational in nature. Furthermore, the evaluation metrics and strategies employed in these studies differ significantly from those typically used in educational contexts. The present discourse fails to present a compelling case regarding the efficacy and practicality of stated methods within the context of higher education. This study aimed to examine multiple {QA} pairs generation approaches in relation to their performance and the efficacy and constraints within the context of higher education. The various approaches encompassed in this study comprise pipeline, joint, multi-task approach. The performance of these approaches under consideration was assessed on three datasets related to distinct courses. The evaluation integrates three automated methods, teacher assessments, and real-world educational evaluations to provide a comprehensive analysis. The comparison of various approaches was conducted by directly assessing their performance using the average scores of different automatic metrics on three datasets. The results of the teachers and real educational evaluation indicate that the assessments generated were beneficial in enhancing the understanding of concepts and overall performance of students. The implications of the findings from this study hold significant importance in enhancing the efficacy of {QA} pair generation tools within the context of higher education.},
	pages = {100252},
	journaltitle = {Computers and Education: Artificial Intelligence},
	shortjournal = {Computers and Education: Artificial Intelligence},
	author = {Ling, Jintao and Afzaal, Muhammad},
	urldate = {2025-04-07},
	date = {2024-06-01},
	keywords = {not really important, can be important, Automatic evaluation, Higher education, Pre-trained language model, Question-answer pairs generation, Real-educational evaluation},
	file = {PDF:/home/malde/Zotero/storage/VK6U4D6T/Ling und Afzaal - 2024 - Automatic question-answer pairs generation using pre-trained large language models in higher educati.pdf:application/pdf;ScienceDirect Snapshot:/home/malde/Zotero/storage/LZSDHA6Z/S2666920X24000559.html:text/html},
}

@inproceedings{scaria_how_2024,
	location = {Mexico City, Mexico},
	title = {How Good are Modern {LLMs} in Generating Relevant and High-Quality Questions at Different Bloom`s Skill Levels for Indian High School Social Science Curriculum?},
	url = {https://aclanthology.org/2024.bea-1.1/},
	abstract = {The creation of pedagogically effective questions is a challenge for teachers and requires significant time and meticulous planning, especially in resource-constrained economies. For example, in India, assessments for social science in high schools are characterized by rote memorization without regard to higher-order skill levels. Automated educational question generation ({AEQG}) using large language models ({LLMs}) has the potential to help teachers develop assessments at scale. However, it is important to evaluate the quality and relevance of these questions. In this study, we examine the ability of different {LLMs} (Falcon 40B, Llama2 70B, Palm 2, {GPT} 3.5, and {GPT} 4) to generate relevant and high-quality questions of different cognitive levels, as defined by Bloom`s taxonomy. We prompt each model with the same instructions and different contexts to generate 510 questions in the social science curriculum of a state educational board in India. Two human experts used a nine-item rubric to assess linguistic correctness, pedagogical relevance and quality, and adherence to Bloom`s skill levels. Our results showed that 91.56\% of the {LLM}-generated questions were relevant and of high quality. This suggests that {LLMs} can generate relevant and high-quality questions at different cognitive levels, making them useful for creating assessments for scaling education in resource-constrained economies.},
	eventtitle = {{BEA} 2024},
	pages = {1--10},
	booktitle = {Proceedings of the 19th Workshop on Innovative Use of {NLP} for Building Educational Applications ({BEA} 2024)},
	publisher = {Association for Computational Linguistics},
	author = {Scaria, Nicy and Chenna, Suma Dharani and Subramani, Deepak},
	editor = {Kochmar, Ekaterina and Bexte, Marie and Burstein, Jill and Horbach, Andrea and Laarmann-Quante, Ronja and Tack, Anaïs and Yaneva, Victoria and Yuan, Zheng},
	urldate = {2025-04-07},
	date = {2024-06},
	keywords = {can be important},
	file = {Full Text PDF:/home/malde/Zotero/storage/SC46AXGV/Scaria et al. - 2024 - How Good are Modern LLMs in Generating Relevant and High-Quality Questions at Different Bloom`s Skil.pdf:application/pdf},
}

@article{cheng_treequestion_2024,
	title = {{TreeQuestion}: Assessing Conceptual Learning Outcomes with {LLM}-Generated Multiple-Choice Questions},
	volume = {8},
	url = {https://dl.acm.org/doi/10.1145/3686970},
	doi = {10.1145/3686970},
	shorttitle = {{TreeQuestion}},
	abstract = {The advances of generative {AI} have posed a challenge for using open-ended questions to assess conceptual learning outcomes, as it is increasingly common for students to use tools like {ChatGPT} to generate long textual answers. However, teachers still have to spend substantial time reading the answers and inferring students' learning outcomes. We present {TreeQuestion}, a human-in-the-loop system designed to help teachers create a set of multiple-choice questions to assess students' conceptual learning outcomes. When a teacher seeks to assess students' comprehension of specific concepts, {TreeQuestion} taps into the wealth of knowledge embedded within large language models and generates a set of multiple-choice questions organized in a tree-like structure. We evaluated {TreeQuestion} with 96 students and 10 teachers. Results indicated that students achieved similar performance in multiple-choice questions generated by {TreeQuestion} and open-ended questions graded by teachers. Meanwhile, {TreeQuestion} could reduce teachers' efforts in creating and grading the multiple-choice questions in contrast to manually generated open-ended questions. We estimate that in a hypothetical class with 20 students, using multiple-choice questions from {TreeQuestion} may require only 4.6\% of the time compared to open-ended questions for assessing learning outcomes.},
	pages = {431:1--431:29},
	issue = {{CSCW}2},
	journaltitle = {Proc. {ACM} Hum.-Comput. Interact.},
	author = {Cheng, Zirui and Xu, Jingfei and Jin, Haojian},
	urldate = {2025-04-07},
	date = {2024-11-08},
	keywords = {can be important},
	file = {Full Text PDF:/home/malde/Zotero/storage/TTCMDPUK/Cheng et al. - 2024 - TreeQuestion Assessing Conceptual Learning Outcomes with LLM-Generated Multiple-Choice Questions.pdf:application/pdf},
}

@article{al_faraby_analysis_2024,
	title = {Analysis of {LLMs} for educational question classification and generation},
	volume = {7},
	issn = {2666-920X},
	url = {https://www.sciencedirect.com/science/article/pii/S2666920X24001012},
	doi = {10.1016/j.caeai.2024.100298},
	abstract = {Large language models ({LLMs}) like {ChatGPT} have shown promise in generating educational content, including questions. This study evaluates the effectiveness of {LLMs} in classifying and generating educational-type questions. We assessed {ChatGPT}'s performance using a dataset of 4,959 user-generated questions labeled into ten categories, employing various prompting techniques and aggregating results with a voting method to enhance robustness. Additionally, we evaluated {ChatGPT}'s accuracy in generating type-specific questions from 100 reading sections sourced from five online textbooks, which were manually reviewed by human evaluators. We also generated questions based on learning objectives and compared their quality to those crafted by human experts, with evaluations by experts and crowdsourced participants. Our findings reveal that {ChatGPT} achieved a macro-average F1-score of 0.57 in zero-shot classification, improving to 0.70 when combined with a Random Forest classifier using embeddings. The most effective prompting technique was zero-shot with added definitions, while few-shot and few-shot + Chain of Thought approaches underperformed. The voting method enhanced robustness in classification. In generating type-specific questions, {ChatGPT}'s accuracy was lower than anticipated. However, quality differences between {ChatGPT}-generated and human-generated questions were not statistically significant, indicating {ChatGPT}'s potential for educational content creation. This study underscores the transformative potential of {LLMs} in educational practices. By effectively classifying and generating high-quality educational questions, {LLMs} can reduce the workload on educators and enable personalized learning experiences.},
	pages = {100298},
	journaltitle = {Computers and Education: Artificial Intelligence},
	shortjournal = {Computers and Education: Artificial Intelligence},
	author = {Al Faraby, Said and Romadhony, Ade and {Adiwijaya}},
	urldate = {2025-04-07},
	date = {2024-12-01},
	keywords = {{LLMs}, Question classification, Question generation, can be important, Graesser's taxonomy},
	file = {PDF:/home/malde/Zotero/storage/2VRMJRCA/Al Faraby et al. - 2024 - Analysis of LLMs for educational question classification and generation.pdf:application/pdf;ScienceDirect Snapshot:/home/malde/Zotero/storage/5KR6PKJM/S2666920X24001012.html:text/html},
}

@article{yang_heuristic_2024,
	title = {Heuristic question sequence generation based on retrieval augmentation},
	volume = {1},
	issn = {30069599, 30069602},
	url = {https://elder.yandypress.com/index.php/3006-9599/article/view/19},
	doi = {10.46690/elder.2024.02.03},
	pages = {72--81},
	number = {2},
	journaltitle = {Education and Lifelong Development Research},
	shortjournal = {Educ. Lifelong Dev. Res.},
	author = {Yang, Zhihao and Zhu, Zhengzhou},
	urldate = {2025-04-07},
	date = {2024-06-25},
	keywords = {not really important, can be important},
	file = {PDF:/home/malde/Zotero/storage/H8PAENW5/Yang und Zhu - 2024 - Heuristic question sequence generation based on retrieval augmentation.pdf:application/pdf},
}

@inproceedings{vu_chatgpt-based_2024,
	location = {New York, {NY}, {USA}},
	title = {A {ChatGPT}-based approach for questions generation in higher education},
	isbn = {979-8-4007-0547-2},
	url = {https://dl.acm.org/doi/10.1145/3643479.3662056},
	doi = {10.1145/3643479.3662056},
	series = {{AIQAM} '24},
	abstract = {Large language models have been widely applied in many aspects of real life, bringing significant efficiency to businesses and offering distinctive user experiences. In this paper, we focus on exploring the application of {ChatGPT}, a chatbot based on a large language model, to support higher educator in generating quiz questions and assessing learners. Specifically, we explore interactive prompting patterns to design an optimal {AI}-powered question bank creation process. The generated questions are evaluated through a "Blind test" survey sent to various stakeholders including lecturers and learners. Initial results at the Banking Academy of Vietnam are relatively promising, suggesting a potential direction to streamline the time and effort involved in assessing learners at higher education institutes.},
	pages = {13--18},
	booktitle = {Proceedings of the 1st {ACM} Workshop on {AI}-Powered Q\&A Systems for Multimedia},
	publisher = {Association for Computing Machinery},
	author = {Vu, Sinh Trong and Truong, Huong Thu and Do, Oanh Tien and Le, Tu Anh and Mai, Tai Tan},
	urldate = {2025-04-07},
	date = {2024-06-10},
	keywords = {can be important},
	file = {Full Text PDF:/home/malde/Zotero/storage/TJHQHHJY/Vu et al. - 2024 - A ChatGPT-based approach for questions generation in higher education.pdf:application/pdf},
}

@inproceedings{doughty_comparative_2024,
	location = {New York, {NY}, {USA}},
	title = {A Comparative Study of {AI}-Generated ({GPT}-4) and Human-crafted {MCQs} in Programming Education},
	isbn = {979-8-4007-1619-5},
	url = {https://dl.acm.org/doi/10.1145/3636243.3636256},
	doi = {10.1145/3636243.3636256},
	series = {{ACE} '24},
	abstract = {There is a constant need for educators to develop and maintain effective up-to-date assessments. While there is a growing body of research in computing education on utilizing large language models\&nbsp;({LLMs}) in generation and engagement with coding exercises, the use of {LLMs} for generating programming {MCQs} has not been extensively explored. We analyzed the capability of {GPT}-4 to produce multiple-choice questions ({MCQs}) aligned with specific learning objectives ({LOs}) from Python programming classes in higher education. Specifically, we developed an {LLM}-powered ({GPT}-4) system for generation of {MCQs} from high-level course context and module-level {LOs}. We evaluated 651 {LLM}-generated and 449 human-crafted {MCQs} aligned to 246 {LOs} from 6 Python courses. We found that {GPT}-4 was capable of producing {MCQs} with clear language, a single correct choice, and high-quality distractors. We also observed that the generated {MCQs} appeared to be well-aligned with the {LOs}. Our findings can be leveraged by educators wishing to take advantage of the state-of-the-art generative models to support {MCQ} authoring efforts.},
	pages = {114--123},
	booktitle = {Proceedings of the 26th Australasian Computing Education Conference},
	publisher = {Association for Computing Machinery},
	author = {Doughty, Jacob and Wan, Zipiao and Bompelli, Anishka and Qayum, Jubahed and Wang, Taozhi and Zhang, Juran and Zheng, Yujia and Doyle, Aidan and Sridhar, Pragnya and Agarwal, Arav and Bogart, Christopher and Keylor, Eric and Kultur, Can and Savelka, Jaromir and Sakr, Majd},
	urldate = {2025-04-07},
	date = {2024-01-29},
	keywords = {important},
	file = {Full Text PDF:/home/malde/Zotero/storage/KWQSSFV7/Doughty et al. - 2024 - A Comparative Study of AI-Generated (GPT-4) and Human-crafted MCQs in Programming Education.pdf:application/pdf},
}

@article{blobstein_angel_2023,
	title = {Angel: A New Generation Tool for Learning Material based Questions and Answers},
	url = {https://gaied.org/neurips2023/files/9/9_paper.pdf},
	abstract = {Creating high quality questions and answers for educational purposes continues to be a challenge for educators and publishers. Past attempts to address this through automatic generation have shown limited abilities to generate questions targeting high cognitive levels, control question complexity and difficulty, or create adequate question-answer pairs. We take first steps toward addressing these limitations by introducing a new approach, named Angel, informed by recent developments in Large Language Models and Generative {AI}. Relying on advanced prompting techniques, automatic curation, and the incorporation of educational theory into prompts, Angel focuses on generating question answer pairs of varied difficulty while targeting higher cognitive levels. Questions and answers are automatically generated based on a textbook extract, with Bloom Taxonomy serving as a guide to the creation of questions addressing a diverse set of learning objectives. Our experiments compare Angel to several baselines and demonstrate the potential of informed generative models to create high-quality question answer pairs that cover a diverse range of cognitive skills.},
	journaltitle = {{NeurIPS}'23 Workshop on Generative {AI} for Education},
	author = {Blobstein, Ariel and Izmaylov, Daniel and Yifal, Tal and Levy, Michal and Segal, Avi},
	urldate = {2025-04-10},
	date = {2023},
	langid = {english},
	keywords = {limitation/loser},
	file = {PDF:/home/malde/Zotero/storage/PZPIBUWF/Blobstein et al. - Angel A New Generation Tool for Learning Material based Questions and Answers.pdf:application/pdf},
}

@inproceedings{duong-trung_bloomllm_2024,
	location = {Cham},
	title = {{BloomLLM}: Large Language Models Based Question Generation Combining Supervised Fine-Tuning and Bloom’s Taxonomy},
	isbn = {978-3-031-72312-4},
	doi = {10.1007/978-3-031-72312-4_11},
	shorttitle = {{BloomLLM}},
	abstract = {Adaptive assessment is challenging, and considering various competence levels and their relations makes it even more complex. Nevertheless, recent developments in artificial intelligence ({AI}) provide new means of addressing these relevant issues. In this paper, we introduce {BloomLLM}, a novel adaptation of Large Language Models ({LLMs}) specifically designed to enhance the generation of educational content in alignment with Bloom’s Revised Taxonomy. {BloomLLM} performs well across all levels of competencies by providing meaningful, semantically connected questions. It is achieved by addressing the challenges of foundational {LLMs}, such as lack of semantic interdependence of levels and increased hallucination, which often result in unrealistic and impractical questions. {BloomLLM}, fine-tuned on {ChatGPT}-3.5-turbo, was developed by fine-tuning 1026 questions spanning 29 topics in two master courses during the winter semester 2023. The model’s performance, outpacing {ChatGPT}-4, even with varied prompting strategies, marks a significant advancement in applying generative {AI} in education. We have publicly made the {BloomLLM} codes and training datasets available to promote transparency and reproducibility.},
	pages = {93--98},
	booktitle = {Technology Enhanced Learning for Inclusive and Equitable Quality Education},
	publisher = {Springer Nature Switzerland},
	author = {Duong-Trung, Nghia and Wang, Xia and Kravčík, Miloš},
	editor = {Ferreira Mello, Rafael and Rummel, Nikol and Jivet, Ioana and Pishtari, Gerti and Ruipérez Valiente, José A.},
	urldate = {2025-04-10},
	date = {2024},
	langid = {english},
	keywords = {{ChatGPT}, {LLMs}, can be important, Bloom’s Revised Taxonomy, {BloomLLM}, Supervised Fine-Tuning},
	file = {Full Text PDF:/home/malde/Zotero/storage/PT99GYUP/Duong-Trung et al. - 2024 - BloomLLM Large Language Models Based Question Generation Combining Supervised Fine-Tuning and Bloom.pdf:application/pdf},
}

@article{zhuge_twinstar_2025,
	title = {{TwinStar}: A Novel Design for Enhanced Test Question Generation Using Dual-{LLM} Engine},
	volume = {15},
	rights = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2076-3417},
	url = {https://www.mdpi.com/2076-3417/15/6/3055},
	doi = {10.3390/app15063055},
	shorttitle = {{TwinStar}},
	abstract = {In light of the remarkable success of large language models ({LLMs}) in natural language understanding and generation, a trend of applying {LLMs} to professional domains with specialized requirements stimulates interest across various fields. It is desirable to further understand the level of intelligence that can be achieved by {LLMs} in solving domain-specific problems, as well as the resources that need to be invested accordingly. This paper studies the problem of generating high-quality test questions with specified knowledge points and target cognitive levels in {AI}-assisted teaching and learning. Our study shows that {LLMs}, even those as immense as {GPT}-4 or Bard, can hardly fulfill the design objectives, lacking clear focus on cognitive levels pertaining to specific knowledge points. In this paper, we explore the opportunity of enhancing the capability of {LLMs} through system design, instead of training models with substantial domain-specific data, consuming mass computing and memory resources. We propose a novel design scheme that orchestrates a dual-{LLM} engine, consisting of a question generation model and a cognitive-level evaluation model, built with fine-tuned, lightweight baseline models and prompting technology to generate high-quality test questions. The experimental results show that the proposed design framework, {TwinStar}, outperforms the state-of-the-art {LLMs} for effective test question generation in terms of cognitive-level adherence and knowledge relevance. {TwinStar} implemented with {ChatGLM}2-6B improves the cognitive-level adherence by almost 50\% compared to Bard and 21\% compared to {GPT}-4.0. The overall improvement in the quality of test questions generated by {TwinStar} reaches 12.0\% compared to Bard and 2\% compared with {GPT}-4.0 while our {TwinStar} implementation consumes only negligible memory space compared with that of {GPT}-4.0. An implementation of {TwinStar} using {LLaMA}2-13B shows a similar trend of improvement.},
	pages = {3055},
	number = {6},
	journaltitle = {Applied Sciences},
	author = {Zhuge, Qingfeng and Wang, Han and Chen, Xuyang},
	urldate = {2025-04-09},
	date = {2025-01},
	langid = {english},
	note = {Number: 6
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {large language model, can be important, automated test generation, fine-tuning, prompting},
	file = {Full Text PDF:/home/malde/Zotero/storage/26AUC74U/Zhuge et al. - 2025 - TwinStar A Novel Design for Enhanced Test Question Generation Using Dual-LLM Engine.pdf:application/pdf},
}

@article{hang_mcqgen_2024,
	title = {{MCQGen}: A Large Language Model-Driven {MCQ} Generator for Personalized Learning},
	volume = {12},
	issn = {2169-3536},
	url = {https://ieeexplore.ieee.org/document/10577164/},
	doi = {10.1109/ACCESS.2024.3420709},
	shorttitle = {{MCQGen}},
	abstract = {In the dynamic landscape of contemporary education, the evolution of teaching strategies such as blended learning and flipped classrooms has highlighted the need for efficient and effective generation of multiple-choice questions ({MCQs}). To address this, we introduce {MCQGen}, a novel generative artificial intelligence framework designed for the automated creation of {MCQs}. {MCQGen} uniquely integrates a large language model ({LLM}) with retrieval-augmented generation and advanced prompt engineering techniques, drawing from an extensive external knowledge base. This integration significantly enhances the ability of the {LLM} to produce educationally relevant questions that align with both the goals of educators and the diverse learning needs of students. The framework employs innovative prompt engineering, combining chain-of-thought and self-refine prompting techniques, to enhance the performance of the {LLM}. This process leads to the generation of questions that are not only contextually relevant and challenging but also reflective of common student misconceptions, contributing effectively to personalized learning experiences and enhancing student engagement and understanding. Our extensive evaluations showcase the effectiveness of {MCQGen} in producing high-quality {MCQs} for various educational needs and learning styles. The framework demonstrates its potential to significantly reduce the time and expertise required for {MCQ} creation, marking its practical utility in modern education. In essence, {MCQGen} offers an innovative and robust solution for the automated generation of {MCQs}, enhancing personalized learning in the digital era.},
	pages = {102261--102273},
	journaltitle = {{IEEE} Access},
	author = {Hang, Ching Nam and Wei Tan, Chee and Yu, Pei-Duo},
	urldate = {2025-04-09},
	date = {2024},
	keywords = {Large language models, Education, multiple-choice questions, Semantics, Task analysis, can be important, prompt engineering, retrieval-augmented generation, personalized learning, Data augmentation, Information retrieval, Knowledge based systems, Knowledge engineering, Problem-solving, Testing},
	file = {Full Text PDF:/home/malde/Zotero/storage/GCUFNF63/Hang et al. - 2024 - MCQGen A Large Language Model-Driven MCQ Generator for Personalized Learning.pdf:application/pdf},
}

@inproceedings{lamsiyah_fine-tuning_2024,
	location = {Cham},
	title = {Fine-Tuning a Large Language Model with Reinforcement Learning for Educational Question Generation},
	isbn = {978-3-031-64302-6},
	doi = {10.1007/978-3-031-64302-6_30},
	abstract = {Educational Natural Language Generation ({EduQG}) aims to automatically generate educational questions from textual content, which is crucial for the expansion of online education. Prior research in {EduQG} has predominantly relied on cross-entropy loss for training, which can lead to issues such as exposure bias and inconsistencies between training and testing metrics. To mitigate this issue, we propose a reinforcement learning ({RL}) based large language model ({LLM}) for educational question generation. In particular, we fine-tune the Google {FLAN}-T5 model using a mixed objective function that combines cross-entropy and {RL} losses to ensure the generation of questions that are syntactically and semantically accurate. The experimental results on the {SciQ} question generation dataset show that the proposed method is competitive with current state-of-the-art systems in terms of predictive performance and linguistic quality.},
	pages = {424--438},
	booktitle = {Artificial Intelligence in Education},
	publisher = {Springer Nature Switzerland},
	author = {Lamsiyah, Salima and El Mahdaouy, Abdelkader and Nourbakhsh, Aria and Schommer, Christoph},
	editor = {Olney, Andrew M. and Chounta, Irene-Angelica and Liu, Zitao and Santos, Olga C. and Bittencourt, Ig Ibert},
	urldate = {2025-04-10},
	date = {2024},
	langid = {english},
	keywords = {can be important, Large Language Model, Educational Question Generation, Google {FLAN}-T5, Reinforcement Learning, Self-Critical Sequence Training},
	file = {Full Text PDF:/home/malde/Zotero/storage/DQPVAVPQ/Lamsiyah et al. - 2024 - Fine-Tuning a Large Language Model with Reinforcement Learning for Educational Question Generation.pdf:application/pdf},
}

@inproceedings{li_planning_2024,
	location = {Bangkok, Thailand},
	title = {Planning First, Question Second: An {LLM}-Guided Method for Controllable Question Generation},
	url = {https://aclanthology.org/2024.findings-acl.280/},
	doi = {10.18653/v1/2024.findings-acl.280},
	shorttitle = {Planning First, Question Second},
	abstract = {In the field of education, for better assessment of students' abilities, generated questions often need to meet experts' requirements, indicating the need for controllable question generation ({CQG}). However, current {CQG} methods mainly focus on difficulty control, neglecting the control of question content and assessed abilities, which are also crucial in educational {QG}. In this paper, we propose an {LLM}-guided method {PFQS} (for Planning First, Question Second), which utilizes Llama 2 to generate an answer plan and then generates questions based on it. The plan not only includes candidate answers but also integrates {LLM}`s understanding and multiple requirements, which make question generation simple and controllable. We evaluate our approach on the {FairytaleQA} dataset, a well-structured {QA} dataset derived from child-friendly storybooks. In the dataset, the attribute label represents content control, while the local\_or\_sum and ex\_or\_im labels denote difficulty control. Experimental results demonstrate that our approach outperforms previous state-of-the-art results and achieves better consistency with requirements compared to prompt-based method. Further application of our method to Llama 2 and Mistral also leads to improved requirement consistency in a zero-shot setting.},
	eventtitle = {Findings 2024},
	pages = {4715--4729},
	booktitle = {Findings of the Association for Computational Linguistics: {ACL} 2024},
	publisher = {Association for Computational Linguistics},
	author = {Li, Kunze and Zhang, Yu},
	editor = {Ku, Lun-Wei and Martins, Andre and Srikumar, Vivek},
	urldate = {2025-04-10},
	date = {2024-08},
	keywords = {not really important, limitation/loser},
	file = {Full Text PDF:/home/malde/Zotero/storage/WPDXXDWK/Li und Zhang - 2024 - Planning First, Question Second An LLM-Guided Method for Controllable Question Generation.pdf:application/pdf},
}

@misc{luo_systematic_2023,
	title = {Systematic Assessment of Factual Knowledge in Large Language Models},
	url = {http://arxiv.org/abs/2310.11638},
	doi = {10.48550/arXiv.2310.11638},
	abstract = {Previous studies have relied on existing question-answering benchmarks to evaluate the knowledge stored in large language models ({LLMs}). However, this approach has limitations regarding factual knowledge coverage, as it mostly focuses on generic domains which may overlap with the pretraining data. This paper proposes a framework to systematically assess the factual knowledge of {LLMs} by leveraging knowledge graphs ({KGs}). Our framework automatically generates a set of questions and expected answers from the facts stored in a given {KG}, and then evaluates the accuracy of {LLMs} in answering these questions. We systematically evaluate the state-of-the-art {LLMs} with {KGs} in generic and specific domains. The experiment shows that {ChatGPT} is consistently the top performer across all domains. We also find that {LLMs} performance depends on the instruction finetuning, domain and question complexity and is prone to adversarial context.},
	number = {{arXiv}:2310.11638},
	publisher = {{arXiv}},
	author = {Luo, Linhao and Vu, Thuy-Trang and Phung, Dinh and Haffari, Gholamreza},
	urldate = {2025-04-10},
	date = {2023-10-30},
	eprinttype = {arxiv},
	eprint = {2310.11638 [cs]},
	keywords = {Computer Science - Computation and Language, can be important},
	file = {Preprint PDF:/home/malde/Zotero/storage/IXMQMDVM/Luo et al. - 2023 - Systematic Assessment of Factual Knowledge in Large Language Models.pdf:application/pdf;Snapshot:/home/malde/Zotero/storage/Z2EXTLIL/2310.html:text/html},
}

@inproceedings{biancini_multiple-choice_2024,
	location = {New York, {NY}, {USA}},
	title = {Multiple-Choice Question Generation Using Large Language Models: Methodology and Educator Insights},
	isbn = {979-8-4007-0466-6},
	url = {https://dl.acm.org/doi/10.1145/3631700.3665233},
	doi = {10.1145/3631700.3665233},
	series = {{UMAP} Adjunct '24},
	shorttitle = {Multiple-Choice Question Generation Using Large Language Models},
	abstract = {Integrating Artificial Intelligence ({AI}) in educational settings has brought new learning approaches, transforming the practices of both students and educators. Among the various technologies driving this transformation, Large Language Models ({LLMs}) have emerged as powerful tools for creating educational materials and question answering, but there are still space for new applications. Educators commonly use Multiple-Choice Questions ({MCQs}) to assess student knowledge, but manually generating these questions is resource-intensive and requires significant time and cognitive effort. In our opinion, {LLMs} offer a promising solution to these challenges. This paper presents a novel comparative analysis of three widely known {LLMs} - Llama 2, Mistral, and {GPT}-3.5 - to explore their potential for creating informative and challenging {MCQs}. In our approach, we do not rely on the knowledge of the {LLM}, but we inject the knowledge into the prompt to contrast the hallucinations, giving the educators control over the test’s source text, too. Our experiment involving 21 educators shows that {GPT}-3.5 generates the most effective {MCQs} across several known metrics. Additionally, it shows that there is still some reluctance to adopt {AI} in the educational field. This study sheds light on the potential of {LLMs} to generate {MCQs} and improve the educational experience, providing valuable insights for the future.},
	pages = {584--590},
	booktitle = {Adjunct Proceedings of the 32nd {ACM} Conference on User Modeling, Adaptation and Personalization},
	publisher = {Association for Computing Machinery},
	author = {Biancini, Giorgio and Ferrato, Alessio and Limongelli, Carla},
	urldate = {2025-04-10},
	date = {2024-06-28},
	keywords = {not really important, limitation/loser},
	file = {Full Text PDF:/home/malde/Zotero/storage/9HEL8L7H/Biancini et al. - 2024 - Multiple-Choice Question Generation Using Large Language Models Methodology and Educator Insights.pdf:application/pdf},
}

@misc{guo_survey_2024,
	title = {A Survey on Neural Question Generation: Methods, Applications, and Prospects},
	url = {http://arxiv.org/abs/2402.18267},
	doi = {10.48550/arXiv.2402.18267},
	shorttitle = {A Survey on Neural Question Generation},
	abstract = {In this survey, we present a detailed examination of the advancements in Neural Question Generation ({NQG}), a field leveraging neural network techniques to generate relevant questions from diverse inputs like knowledge bases, texts, and images. The survey begins with an overview of {NQG}'s background, encompassing the task's problem formulation, prevalent benchmark datasets, established evaluation metrics, and notable applications. It then methodically classifies {NQG} approaches into three predominant categories: structured {NQG}, which utilizes organized data sources, unstructured {NQG}, focusing on more loosely structured inputs like texts or visual content, and hybrid {NQG}, drawing on diverse input modalities. This classification is followed by an in-depth analysis of the distinct neural network models tailored for each category, discussing their inherent strengths and potential limitations. The survey culminates with a forward-looking perspective on the trajectory of {NQG}, identifying emergent research trends and prospective developmental paths. Accompanying this survey is a curated collection of related research papers, datasets and codes, systematically organized on Github, providing an extensive reference for those delving into {NQG}.},
	number = {{arXiv}:2402.18267},
	publisher = {{arXiv}},
	author = {Guo, Shasha and Liao, Lizi and Li, Cuiping and Chua, Tat-Seng},
	urldate = {2025-04-10},
	date = {2024-05-07},
	eprinttype = {arxiv},
	eprint = {2402.18267 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence, can be important},
	file = {Preprint PDF:/home/malde/Zotero/storage/4LX6U7LW/Guo et al. - 2024 - A Survey on Neural Question Generation Methods, Applications, and Prospects.pdf:application/pdf;Snapshot:/home/malde/Zotero/storage/8ZFDRQ6G/2402.html:text/html},
}

@inproceedings{moore_automatic_2024,
	location = {Cham},
	title = {An Automatic Question Usability Evaluation Toolkit},
	isbn = {978-3-031-64299-9},
	doi = {10.1007/978-3-031-64299-9_3},
	abstract = {Evaluating multiple-choice questions ({MCQs}) involves either labor-intensive human assessments or automated methods that prioritize readability, often overlooking deeper question design flaws. To address this issue, we introduce the Scalable Automatic Question Usability Evaluation Toolkit ({SAQUET}), an open-source tool that leverages the Item-Writing Flaws ({IWF}) rubric for a comprehensive and automated quality evaluation of {MCQs}. By harnessing the latest in large language models such as {GPT}-4, advanced word embeddings, and Transformers designed to analyze textual complexity, {SAQUET} effectively pinpoints and assesses a wide array of flaws in {MCQs}. We first demonstrate the discrepancy between commonly used automated evaluation metrics and the human assessment of {MCQ} quality. Then we evaluate {SAQUET} on a diverse dataset of {MCQs} across the five domains of Chemistry, Statistics, Computer Science, Humanities, and Healthcare, showing how it effectively distinguishes between flawed and flawless questions, providing a level of analysis beyond what is achievable with traditional metrics. With an accuracy rate of over 94\% in detecting the presence of flaws identified by human evaluators, our findings emphasize the limitations of existing evaluation methods and showcase potential in improving the quality of educational assessments.},
	pages = {31--46},
	booktitle = {Artificial Intelligence in Education},
	publisher = {Springer Nature Switzerland},
	author = {Moore, Steven and Costello, Eamon and Nguyen, Huy A. and Stamper, John},
	editor = {Olney, Andrew M. and Chounta, Irene-Angelica and Liu, Zitao and Santos, Olga C. and Bittencourt, Ig Ibert},
	urldate = {2025-04-23},
	date = {2024},
	langid = {english},
	keywords = {can be important, Multiple-Choice Questions, Question Evaluation, Question Quality},
	file = {Full Text PDF:/home/malde/Zotero/storage/3UBQQIWI/Moore et al. - 2024 - An Automatic Question Usability Evaluation Toolkit.pdf:application/pdf},
}

@inproceedings{horbach_linguistic_2020,
	location = {Marseille, France},
	title = {Linguistic Appropriateness and Pedagogic Usefulness of Reading Comprehension Questions},
	isbn = {979-10-95546-34-4},
	url = {https://aclanthology.org/2020.lrec-1.217/},
	abstract = {Automatic generation of reading comprehension questions is a topic receiving growing interest in the {NLP} community, but there is currently no consensus on evaluation metrics and many approaches focus on linguistic quality only while ignoring the pedagogic value and appropriateness of questions. This paper overcomes such weaknesses by a new evaluation scheme where questions from the questionnaire are structured in a hierarchical way to avoid confronting human annotators with evaluation measures that do not make sense for a certain question. We show through an annotation study that our scheme can be applied, but that expert annotators with some level of expertise are needed. We also created and evaluated two new evaluation data sets from the biology domain for Basque and German, composed of questions written by people with an educational background, which will be publicly released. Results show that manually generated questions are in general both of higher linguistic as well as pedagogic quality and that among the human generated questions, teacher-generated ones tend to be most useful.},
	eventtitle = {{LREC} 2020},
	pages = {1753--1762},
	booktitle = {Proceedings of the Twelfth Language Resources and Evaluation Conference},
	publisher = {European Language Resources Association},
	author = {Horbach, Andrea and Aldabe, Itziar and Bexte, Marie and Lopez de Lacalle, Oier and Maritxalar, Montse},
	editor = {Calzolari, Nicoletta and Béchet, Frédéric and Blache, Philippe and Choukri, Khalid and Cieri, Christopher and Declerck, Thierry and Goggi, Sara and Isahara, Hitoshi and Maegaard, Bente and Mariani, Joseph and Mazo, Hélène and Moreno, Asuncion and Odijk, Jan and Piperidis, Stelios},
	urldate = {2025-05-12},
	date = {2020-05},
	keywords = {can be important},
	file = {Full Text PDF:/home/malde/Zotero/storage/87U9YEV7/Horbach et al. - 2020 - Linguistic Appropriateness and Pedagogic Usefulness of Reading Comprehension Questions.pdf:application/pdf},
}

@inproceedings{elkins_how_2023,
	location = {Cham},
	title = {How Useful Are Educational Questions Generated by Large Language Models?},
	isbn = {978-3-031-36336-8},
	doi = {10.1007/978-3-031-36336-8_83},
	abstract = {Controllable text generation ({CTG}) by large language models has a huge potential to transform education for teachers and students alike. Specifically, high quality and diverse question generation can dramatically reduce the load on teachers and improve the quality of their educational content. Recent work in this domain has made progress with generation, but fails to show that real teachers judge the generated questions as sufficiently useful for the classroom setting; or if instead the questions have errors and/or pedagogically unhelpful content. We conduct a human evaluation with teachers to assess the quality and usefulness of outputs from combining {CTG} and question taxonomies (Bloom’s and a difficulty taxonomy). The results demonstrate that the questions generated are high quality and sufficiently useful, showing their promise for widespread use in the classroom setting.},
	pages = {536--542},
	booktitle = {Artificial Intelligence in Education. Posters and Late Breaking Results, Workshops and Tutorials, Industry and Innovation Tracks, Practitioners, Doctoral Consortium and Blue Sky},
	publisher = {Springer Nature Switzerland},
	author = {Elkins, Sabina and Kochmar, Ekaterina and Serban, Iulian and Cheung, Jackie C. K.},
	editor = {Wang, Ning and Rebolledo-Mendez, Genaro and Dimitrova, Vania and Matsuda, Noboru and Santos, Olga C.},
	date = {2023},
	langid = {english},
	keywords = {can be important, Controllable Text Generation, Personalized Learning, Prompting, Question Generation},
	file = {Full Text PDF:/home/malde/Zotero/storage/KMS7JA4E/Elkins et al. - 2023 - How Useful Are Educational Questions Generated by Large Language Models.pdf:application/pdf},
}
