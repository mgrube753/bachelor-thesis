\section{Introduction}
% \newtodo{Give context for your research, introduce research question, outline your work.}
% \newtodo{Mirroring the expose, what can the readers expect from the thesis? Do not copy the expose text}
% \newtodo{Claims have to be referenced, maybe add a list of contributions.}

This thesis investigates the capabilities of \ac{llms} for the automatic generation and evaluation of educational questions. The focus is on the growing need for effective educational tools that can support teaching and learning.

\subsection{Motivation}

Effective learning often relies on engaging actively with educational content, instead of reading it passively \cite{steuer_i_2021}. One effective teaching strategy is to ask questions about the content that the students are dealing with, which can help them to understand and comprehend the material better \cite{steuer_i_2021}. However, creating questions that are both relevant and cognitively demanding can be a time-consuming task for educators \cite{steuer_i_2021,vu_chatgpt-based_2024}. Many educational materials and textbooks lack sufficient questions, so testing the students' comprehension is limited \cite{steuer_i_2021}.

\ac{llms} have shown remarkable capabilities in understanding and generating human-like text across various domains and tasks \cite{fan_bibliometric_2024,wang_history_2024}. By this, these models have potential to assist educators via \ac{aqg} \cite{vu_chatgpt-based_2024,ling_automatic_2024}. Leveraging \ac{llms} for \ac{aqg} could enable educators to produce a larger volume and wider variety of questions more efficiently \cite{scaria_how_2024,moore_automatic_2024,hang_mcqgen_2024}, allowing them to focus more time on direct student interaction \cite{doughty_comparative_2024,al_faraby_analysis_2024} and providing personalized feedback \cite{ling_automatic_2024}.

\subsection{Role of AI in Education}

\object{Originality of the Problem} Existing \ac{aqg} systems for educational purposes have certain limitations. Many prior approaches focused on the linguistic quality while disregarding the pedagogical value and appropriateness of questions \cite{horbach_linguistic_2020,steuer_i_2021}. Outdated models \cite{cheng_treequestion_2024,elkins_how_2023,bhowmick_automating_2023} or certain \ac{slms} \cite{luo_systematic_2023} were often used, potentially due to power and cost constraints \cite{zhuge_twinstar_2025,vu_chatgpt-based_2024}. Furthermore, the rising reasoning abilities of \ac{llms} were not leveraged in the literature. 
A significant research gap exists regarding the creation of cognitively challenging questions, particularly the effective implementation and scoring of questions aligned with higher levels of Bloom's Taxonomy. While \ac{llms} such as ChatGPT \cite{openai_introducing_2022} are good at generating questions for lower cognitive levels, they have limitations at the higher levels, often generating oversimplified or unrealistic questions \cite{duong-trung_bloomllm_2024,elkins_how_2023,scaria_automated_2024}. 

% \object{Relevance of the Problem} The development of effective educational tools is increasingly important to support both teaching and learning. Automating the question generation process can significantly reduce the workload for this process, leading to saving the educators' time \cite{biancini_multiple-choice_2024,hang_mcqgen_2024,vu_chatgpt-based_2024,cheng_treequestion_2024}. By leveraging \ac{llms} for this purpose, the systems can support the scalability of educational content, creating question banks for various learning modules \cite{vu_chatgpt-based_2024} and enabling personalized education for varying student needs and learning styles \cite{hang_mcqgen_2024,yang_heuristic_2024}.

\object{Relevance of the Problem} The development of effective educational tools is increasingly important to support both teaching and learning. Automating question generation, particularly for educational assessments, can significantly reduce the workload for educators, leading to saving the educators' time \cite{biancini_multiple-choice_2024,hang_mcqgen_2024,vu_chatgpt-based_2024,cheng_treequestion_2024}. By leveraging \ac{llms}, these systems can support the creation of scalable educational content, such as question banks for various learning modules \cite{vu_chatgpt-based_2024}. Furthermore, they have potential to enable personalized education for varying needs and learning styles, enhancing student engagement and understanding \cite{hang_mcqgen_2024}.

\object{Expected Contributions}
This work contributes to the field through offering a systematic evaluation for assessing educational questions generated by \ac{llms}. A key focus is evaluating the adherence of generated questions to certain ISO-OSI model-based source materials, ensuring that the questions can be answered based on the provided content. This research also investigates the capabilities of \ac{llms} in \ac{aqg} while considering different cognitive levels of Bloom's Taxonomy. The evaluation's insights imply guidelines for optimizing prompts for educational purposes, since the prompt design is crucial for the quality of generated questions.

% \begin{itemize}
%     \item A systematic evaluation for assessing \ac{llm}-generated educational questions
%     \item connection between questions and given source material
%     \item Empirical evidence of current \ac{llms}' capabilities across different cognitive levels
%     \item Guidelines for optimizing question generation prompts for educational purposes
%     \item Insights into the practical application of \ac{llms} in educational content creation
% \end{itemize}

% ----------------------------------------------------

% \object{Evaluation Criteria}
% \begin{itemize}
%      \item Cognitive Complexity According to Bloom
%      \item Content Accuracy
%      \item Pedagogical Suitability
%      \item Variability of Generated Questions
% \end{itemize}
% \object{Scope and Limitations}
% \begin{itemize}
%      \item Focus on Text-Based Questions
%      \item Restriction to Technical Educational Content
% \end{itemize}

% \vp

\subsection{Research Questions}

Regarding the identified potential and limitations of \ac{llms} in educational contexts, this thesis aims to provide a comprehensive evaluation of their effectiveness in generating educational questions. To achieve this, the research will be guided by the following questions:

\requ{1}{To what extent do \ac{llms} adhere to the content of diverse provided instructional texts when generating questions?} Evaluating this aspect is crucial to generate reliable educational questions. This involves assessing whether the questions can be answered using the source material \cite{maity_can_2025,moore_assessing_2022,scaria_automated_2024} and measuring the content fidelity based on semantic similarity to reference content \cite{li_planning_2024}. To avoid the generation of misleading or irrelevant information, called \enquote{hallucination}, ensuring that the generated questions are aligned with the instructional text is needed \cite{al_faraby_analysis_2024}.

\requ{2}{How does the relationship between diverse question formats and Bloom's Taxonomy levels influence the pedagogical effectiveness of \ac{llm}-generated questions?} Previous work has pointed out challenges in generating questions for higher Bloom's levels \cite{scaria_automated_2024} and the need of pedagogical evaluation beyond linguistic tests \cite{horbach_linguistic_2020,steuer_i_2021}. This research question aims to explore how specific question formats perform across different cognitive levels of Bloom's revised Taxonomy \cite{krathwohl_revision_2002} and how this influences the usefulness for educational intentions.

% \vp

\subsection{Thesis Outline}

This Bachelor's thesis is organized as follows:
Section \bfref{sec:theoretical-foundations} provides the theoretical foundations necessary to understand the research, including an introduction to Large Language Models, Bloom's Taxonomy, prompting strategies, and evaluation methodologies.
Section \bfref{sec:related-work} reviews related work in the field of \ac{llms} and \ac{aqg}, also focusing Bloom's Taxonomy.
Section \bfref{sec:experiment-design} addresses the experimental design, outlining the experimental setup, and the methodological approach used to evaluate \ac{llms}' capabilities in generating educational questions.
Section \bfref{sec:implementation-and-execution} describes the implementation and execution of the experiments, including the used APIs, the experimental procedure, the promptings, the data generation process and the evaluation methods.
Section \bfref{sec:evaluation} presents the evaluation of results, offering both quantitative and qualitative analyses for both experiments of this work.
Section \bfref{sec:conclusion-and-future-work} concludes the thesis by summarizing key findings, discussing practical implications for education, and also suggesting directions for future research.
