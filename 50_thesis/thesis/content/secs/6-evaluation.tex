\section{Evaluation of Results} \label{sec:evaluation}

This section presents the findings from the two experiments detailed in Section \bfref{sec:experiment-design}. The quantitative and qualitative analyses are discussed, addressing the research questions regarding content fidelity, error propagation, question format generation, and alignment with Bloom's Taxonomy.

% \newtodo{Need some approaches (LLMs) to compare against and provide metrics.}
% \newtodo{While benchmarking, compare to something that already exists, hmmmm.}
% \newtodo{Include an overview of the system's implementation but not too detailed if unnecessary.}

\subsection{First Experiment}
\label{sec:first-experiment}

\object{Null and Alternative Hypotheses} ...

\hypo{0}{The usage of a complex prompt with detailed instructions, in contrast to a common one, will lead to a higher content fidelity of the generated questions with respect to the source material, and also higher final scoring for each question.}

\hypo{0'}{Both the common and complex prompts will lead to a similar content fidelity of the generated questions with respect to the source material, and also similar final scoring for each question.}

% \hypo{2}{The content fidelity with respect to the \ac{llms} varies depending on the language and\,/\,or the correctness of the source material.}

...

\object{Quantitative Analysis}

\begin{itemize}
    \item Semantic Similarity, own calculation of cosine similarity score
    \item LLM-based Content Adherence calculation
\end{itemize}

\object{Qualitative Analysis}

\begin{itemize}
    \item The Exp1 rubric
    \item With Claude 3.7 Sonnet and OpenAI o3 as LLMs as evaluators and expert evaluation, both sampled
\end{itemize}

\object{Regarding H0 and H0'} ...

% \begin{itemize}
%     \item LLM as an Evaluator, 2 runs for better validity
%     \item Sampled Expert Evaluation, here Cap would be suitable
%     \item Using a suitable rubric together, not the \cite{horbach_linguistic_2020} one, better a smaller one with $\leq 5$ categories
%     \item We need high inter-annotator agreement score between \ac{llm} and expert to show validity of the \ac{llm} as an evaluator
%     \item And for re-run: Adherence to the malicious input text (does it follow the manipulated context, or does it use the system knowledge?)
% \end{itemize}

\vp

\subsection{Second Experiment}
\label{sec:second-experiment}

\object{Null and Alternative Hypotheses} ...

\hypo{1}{The combined specification of the Bloom's level and the question type will result in a higher adherence to the respective level than specifying solely the question type or the Bloom's level.}

\hypo{1'}{At least one out of the chosen \ac{llms} will not be able to create questions with higher adherence to the Bloom's level if both specifications are given together.}

\object{Qualitative Analysis}

\begin{itemize}
    \item The Exp2 rubric
    \item With Claude 3.7 Sonnet and OpenAI o3 as LLMs as evaluators and expert evaluation, both sampled
\end{itemize}

\object{Regarding H1 and H1'} ...

% \begin{itemize}
%     \item LLM as an Evaluator, with 2 runs for better validity
%     \item Sampled Expert Evaluation, here Rubach would be suitable
%     \item Rubric by \cite{horbach_linguistic_2020}, but also the refined one by \cite{scaria_how_2024} by replacing \textit{Rephrase} category with \textit{Bloom'sLevel}
%     \item Rubric by \cite{mi_comparative_2024} could be useful as well
%     \item Also need high inter-annotator agreement between \ac{llm} and expert to show validity of the \ac{llm} as an evaluator
% \end{itemize}

\vp