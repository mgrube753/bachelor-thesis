\section{Implementation and Execution} 
\label{sec:implementation-and-execution}

This chapter details the practical implementation of the experimental design outlined in Section \ref{sec:experiment-design}. It covers the technical setup, data generation pipeline, prompt engineering strategies, and evaluation methodologies that address the research questions regarding \ac{llm} capabilities in educational question generation.

\subsection{Technical Implementation}

\object{Development Environment and Tools}
The experimental pipeline was implemented using Python 3.10.14 as the primary development platform, chosen for its robust ecosystem of machine learning libraries and API integration capabilities.

\object{Core System Components}
The system primarily used a comprehensive API integration layer implemented through the \bfhyperref{lst:api-calls}{api\_calls.py} module, which manages interactions with multiple \ac{llm} providers using their official Python SDKs. By this, the OpenAI Python SDK for o3 access\footref{fn:openai-api}, the Anthropic SDK for Claude 3.7 Sonnet integration\footref{fn:anthropic-api}, and the Google AI Python SDK for Gemini 2.5 Flash\footref{fn:google-api} could be integrated properly. 

To manage the needed API configurations, the \bfhyperref{lst:api-config}{api\_config.py} module was implemented, which handles API credentials and initialization. The prompt engineering framework, implemented in \bfhyperref{lst:prompt-utils}{prompt\_utils.py}, provides dynamic prompt generation capabilities based on experimental conditions while integrating templates from the prompts directory. Moreover, the data processing pipeline utilizes core utilities in \bfhyperref{lst:file-utils}{file\_utils.py} to load and save files.

\subsection{Experimental Procedure}

\object{Prompt Engineering Strategy}
The prompt design strategy adheres to established best practices, incorporating role-based prompting, special markers for text and format requirements, and positive constraints. Two distinct prompt categories were developed to address the specific requirements of each experimental phase. Since there are no gold standard questions available, the prompts are based on the zero-shot approach, which allows the \ac{llms} to generate questions based on the provided content without prior examples.

For Experiment 1, two complementary approaches were implemented: a simple prompt strategy detailed in \bfhyperref{lst:exp1-common}{exp1\_common\_prompt.md} provides minimal instructions for \ac{aqg} based on ISO-OSI layer content, while a complex prompt approach outlined in \bfhyperref{lst:exp1-complex}{exp1\_complex\_prompt.md} incorporates role assignment, specific formatting requirements, and critical thinking encouragement to enhance question quality.

\pagebreak

Experiment 2 focuses on Bloom's Taxonomy alignment and question format specification through three specialized prompt variations. The type-focused approach in \bfhyperref{lst:exp2-type}{exp2\_type.md} specifies question formats such as Multiple-Choice and Open-Ended questions without imposing cognitive level constraints. Conversely, the Bloom-focused strategy implemented in \bfhyperref{lst:exp2-bloom}{exp2\_bloom.md} targets specific cognitive levels without restricting format choices. The combined specification approach in \bfhyperref{lst:exp2-both}{exp2\_both.md} integrates both requirements to examine their interaction effects. The description and associated verbs for each Bloom level were adopted by Section \bfref{sec:blooms-taxonomy}, resulting in the \bfhyperref{lst:bloom}{bloom.md}.

\object{Automated Experiment Execution}
The complete experimental pipeline is initialized bia \bfhyperref{lst:main}{main.py}, which coordinates all experimental phases. Several constants and configurations were managed by \bfhyperref{lst:constants}{constants.py}. Prior to experiment execution, the crucial token management was analyzed through the \bfhyperref{lst:check-truncation}{check\_truncation.py} file. This step was needed to validate that the multilingual source material segments did not exceed the 512-token limit for semantic similarity calculation. For this, sentence embedding vectors were created by a cross-lingual RoBERTa model\footnote{\url{https://huggingface.co/T-Systems-onsite/cross-en-de-roberta-sentence-transformer}}, which is a modified sentence transformer \cite{reimers_sentence-bert_2019}. This preprocessing step was essential because the model would later be used for semantic similarity evaluation, so the segments had to be within the token limit. Later on, this file was refined to also analyze the amount of tokens in each generated question.
The question generation procedure for both experiments was managed by \bfhyperref{lst:exp}{exp.py}, which handles the \ac{aqg} for content fidelity and error propagation in Experiment 1 -- 224 questions in total -- via different source materials and prompt variations, and Experiment 2 coordination with 120 questions on varied cognitive and question format specifications to examine Bloom's Taxonomy alignment.

\object{Data Management and Organization}
The resulting data follows a systematic directory structure designed for clarity and reproducibility, as good as it can work with \ac{llms}. Source materials containing ISO-OSI content are organized into separate folders for script, transcript, and Tanenbaum book excerpts, enabling efficient access and processing. The generated questions were stored in distinct text files, while the directory path reflects experiment type, \ac{llm} model, source material, prompt variation, and question identifiers, to conduct automated analysis and manual review. Evaluation data is maintained in structured CSV files containing automated metrics, \ac{llm}-based evaluations, and expert annotations, ensuring comprehensive assessment capabilities.

\subsection{Evaluation Implementation}

\object{Multi-Modal Assessment}
The analysis steps include quantitative and qualitative evaluation of the generated questions. Both approaches, besides the manual qualitative expert-based evaluation, were implemented in \bfhyperref{lst:analysis-quantitative}{analysis\_quantitative.py} and \bfhyperref{lst:analysis-qualitative}{analysis\_qualitative.py}. The quantitative analysis follows semantic similarity evaluation between question and source material embeddings. Furthermore, the \ac{llm}-based content adherence assessment -- returning a value between 0 and 1 -- was conducted to provide another quantitative analysis of the \ac{llms}' content adherence to the source material. The prompt for this manner of quantitative evaluation is shown in \bfhyperref{lst:exp1-adherence-eval}{exp1\_adherence\_eval.md}.

Moreover, the qualitative analysis focused on sampled expert-based and \ac{llm}-based evaluation, focusing the same criteria. To properly evaluate using the desired rubric variations from Tables \bfref{tab:exp1_criteria} and \bfref{tab:exp2_criteria}, the \ac{llm} as an evaluator was conducting two evaluation runs to ensure consistency and reliability across assessments. After running this approach on the generated questions, the final scores were calculated by the mean of the results of this \ac{llm}. Expert human evaluation follows the structured annotation using the same criteria. To establish the reliability of automated evaluation methods, the calculation of inter-annotator agreement via Cohen's Kappa was needed, addressed with the \bfhyperref{lst:agreement}{agreement.py} file.

Whether all analysis results are available, the quantitative and qualitative results are processed by means of the \bfhyperref{lst:eval}{eval.py} file and presented in clear tables and plots. These visualizations enable a clear presentation of the results and facilitate the interpretation of the determined data.

\object{Implementation Challenges and Solutions}
Despite the absence of novel technological creations by e.g.\ training, there were some technical challenges that had to be solved. The token limitation of the chosen RoBERTa model required manual shortening of the layer segment texts (if necessary) to ensure a suitable performance of the transformer model for the sentence embeddings. The multilingual processing challenges were tackled by the careful selection of this cross-lingual sentence transformer, which enabled effective handling of both the German lecture materials and the English textbook content.

The developed \ac{aqg} pipeline proved to be particularly challenging, since it required careful consideration of prompting, model selection, and evaluation methods. In addition to that, the implementation of the effective automation of the generative (excluding DeepSeek) and evaluation processes required a lot of attention to detail. 