\section{Related Work} \label{sec:related-work}

This section explores current research in \ac{llms} and their application to \ac{aqg}, with sections to detail the literature review process and highlight significant contributions in the field.

% \newtodo{Discuss other work in the field aiming this related research problem.}
% \newtodo{Show how my research builds on them, where used for inspiration, and e.g. how approaches may be combined.}
% \newtodo{Mention papers, explain why good or bad, also in comparison with my approach.}
% \newtodo{Try to categorize and group the related work by these, show (dis)advantages using one example.}

\object{Literature Review} To find the fundamental principles and state-of-the-art of the research problem, a literature review was conducted. It was performed in the following way: First, several searches were done with respect to \ac{llms} in common, question generation, particularily focusing on Bloom's Taxonomy, and lastly evaluation methods for both categories. Google Scholar and Elicit were used to find relevant papers. Several search terms were coupled in Google Scholar, such as \textit{LLM, Large Language Model, architecture, question generation, automatic question generation, Bloom's Taxonomy, evaluation, assessment, alignment}. Elicit is a search engine that is designed to help researchers find relevant papers based on writing a specific prompt, instead of a query of keywords.

\object{Review Steps} The review steps were as follows: 
\begin{itemize}
    \item First, the papers were downloaded and stored in Zotero. Then, duplicates were filtered out.
    \item The remaining papers were \textbf{included} or excluded by using the TACID method. Papers were included if they are recent (published from 2021 on) and dealing with LLMs or automated question generation or evaluation methods. If the paper was a preprint instead of a peer-reviewed paper, it needed a comparatively high relevance to be included ($\approx 10$ citations). If a publication was present in a workshop, it needed citations to be included.
    \item Papers were \textbf{excluded} if they were not written in English, not open access, or the journal is predatory\footnote{Journals noted on \url{https://beallslist.net/}, accessed 04/30/2025}. If the focus is explicitly on \ac{slms} or multimodal models, or other domains besides the desired ones, the paper was excluded. 
    \item After the process, the remaining papers were filtered by a deeper analysis of the papers.
    \item In the end, diverse cited and subsequently found papers blog posts by e.g.\ Google were added to the thesis to improve the information space.
\end{itemize}

\subsection{Current LLMs Research}

\hspace{1.5em}\object{Review Challenges at \ac{llms}} Finding the current state-of-the-art of \ac{llms} with the explained review method did not yield recent results, even if papers such as surveys were published in 2024 or 2025. The reason for this is that the field of \ac{llms} is moving so fast, that the review papers become outdated. So, the trend of the models and their releases were followed manually, as captured in Table \bfref{tab:llm_releases}. It shows the most important releases of \ac{llms} in the past years since ChatGPT \cite{openai_introducing_2022}. The table is not complete, but it shows well-known models and their architecture, including \textit{reasoning} capabilities, if these are known to the public. Reasoning means, in the OpenAI o1 system card \cite{openai_openai_2024-1} for instance, the model thinks in a manner of \ac{cot} before responding to the user. The table is sorted by the release date of the models to obtain a timeline and a way to interpret the development of the models.

\pagebreak

\object{Major LLM Developers and Models} The following list details prominent developers in the \ac{llm} space and provides an overview of their key model releases and architectural trends.
\begin{itemize}
    \item \textbf{OpenAI:} With the initial release of ChatGPT \cite{openai_introducing_2022}, which firstly utilized the GPT-3.5 architecture, OpenAI has limited the disclosure of architectural details for its subsequent models. The GPT-4 model \cite{openai_gpt-4_2024} was followed by GPT-4o \cite{openai_gpt-4o_2024}, which outperformed GPT-4. The \enquote{GPT} nomenclature indicates a continuation of the decoder-only transformer architecture, a characteristic of known models up to GPT-3 \cite{brown_language_2020}. OpenAI's current state-of-the-art models emphasize reasoning capabilities, as seen in the o1-mini \cite{openai_openai_2024}, o1 \cite{openai_openai_2024-1}, o3-mini \cite{openai_openai_2025}, and the subsequent o3 and o4-mini \cite{openai_openai_2025-1} releases.

    \item \textbf{Meta:} Meta has provided some architectural insights for its LLaMA series. The first generation \cite{touvron_llama_2023} was based on a modified transformer architecture. LLaMA 2 \cite{touvron_llama_2023-1}, up to the LLaMA 3.3 iteration adopted a decoder-only transformer\footnote{\url{https://github.com/meta-llama/llama-models/blob/main/models/llama3_3/MODEL_CARD.md}, accessed 05/09/2025. Note that auto-regressive means decoder-only}. With LLaMA 4 \cite{meta_ai_llama_2025}, Meta also transitioned to \ac{moe} models, with a maximum of 2 trillion parameters on its third model.

    \item \textbf{Google:} Google's Bard, launched in February 2023, was initially based on the LaMDA architecture \cite{pichai_important_2023,thoppilan_lamda_2022}, making it a decoder-only model. Subsequent fine-tuning led to the Gemini series \cite{team_gemini_2024}, which also started as decoder-only. Gemini 1.5 marked a shift to an \ac{moe} architecture, though still described as decoder-based \cite{team_gemini_2024-1}. Beginning with Gemini 2.0 \cite{pichai_introducing_2024}, Google stopped publishing detailed architectural information. Gemini 2.5 \cite{kavukcuoglu_gemini_2025} further advanced their offerings by fully integrating reasoning capabilities, aligning with current state-of-the-art trends.

    \item \textbf{Anthropic:} Anthropic has maintained the policy of not disclosing detailed architectural information for its Claude series. This applies from the initial Claude 1 \cite{anthropic_introducing_2023} and Claude 2 \cite{anthropic_claude_2023} models, through the Claude 3 family \cite{anthropic_introducing_2024-1}, Claude 3.5 Sonnet \cite{anthropic_introducing_2024}, and up to Claude 3.7 Sonnet \cite{anthropic_claude_2025}. The latter also incorporated reasoning capabilities, making it competitive with other currently leading models.

    \item \textbf{xAI:} xAI initially disclosed that its Grok-1 model utilized an \ac{moe} architecture \cite{xai_open_2024}. However, for subsequent releases, Grok-2 \cite{xai_grok-2_2024} and Grok-3 \cite{xai_grok_2025}, architectural details were not provided, though Grok-3 is a competitive state-of-the-art model with reasoning.

    \item \textbf{DeepSeek AI:} DeepSeek AI offers open-source models, with DeepSeek V3 \cite{deepseek-ai_deepseek-v3_2025} as a popular \ac{moe} model. One month later, DeepSeek R1 \cite{deepseek-ai_deepseek-r1_2025} followed, reportedly using the same architecture but with added reasoning capabilities.

    \item \textbf{Alibaba:} The Alibaba Group has also contributed to the field with models like Qwen 2.5-Max \cite{team_qwen25-max_2025}, an \ac{moe} model. The more recent QwQ-Max \cite{team_thinkthink_2025} builds upon this architecture, incorporating reasoning to compete at the state-of-the-art.
\end{itemize}

\pagebreak

Visible in Table \bfref{tab:llm_releases}, the current state-of-the-art for the given table began from o1 and o3-mini in December 2024 plus the release of DeepSeek V3. January 2025, DeepSeek R1 and Alibaba's Qwen 2.5-Max model without reasoning capabilities were released. February 2025 brought xAI's Grok-3, Anthropic's Claude 3.7 Sonnet and Alibaba's QwQ-Max. In March 2025, the state-of-the-art was reached by a major upgrade in Google Gemini, with the release of Gemini 2.5. Finally, April 2025 featured the LLaMA 4 family by Meta and OpenAI's o3 and o4-mini models. Based on this information pool, the models for the \ac{aqg} evaluation were selected, as to be seen in Section \bfref{sec:experiment-design}. 

\object{Key \ac{llm} Development Trends} As illustrated in Table \bfref{tab:llm_releases}, a discernible trend is the decreasing transparency from developers regarding architectural designs of their \ac{llms}. While some models are known to be transformer-based (e.g.\ decoder-only) or employ an \ac{moe} architecture, and reasoning capabilities are often highlighted, many other details remain undisclosed due to the competitive nature of the field. It is also worth noting that there is a trend towards models with a higher number of parameters, a prevalence of decoder-only architectures from ChatGPT on, and an increasing adoption of \ac{moe} designs, a trend possibly popularized by GPT-4. Although the technical report for GPT-4 \cite{openai_gpt-4_2024} lacks architectural specifications, it is rumored to be an \ac{moe} model\footnote{\url{https://medium.com/@seanbetts/peering-inside-gpt-4-understanding-its-mixture-of-experts-moe-architecture-2a42eb8bdcb3}, accessed 05/09/2025}. Furthermore, a significant trend is the continuous integration of reasoning capabilities into \ac{llms}, with the most recent models being competitive in this area.

\subsection{Question Generation Research}
\label{sec:question-generation-research}

\ac{llms} enable the automation of the time-consuming and demanding procedure of question generation \cite{vu_chatgpt-based_2024}. It has been communicated by the authors \cite{naveed_comprehensive_2024}, that the integration of such models into educational settings offer an opportunity to improve the learning experience, support teachers and encourage the development of educational content. The utilization of \ac{llms} has the potential to assist educators in the creation of diverse and inclusive materials with respect to education. This results in freeing up time for educators to focus on teaching and interacting with students.

\object{Focusing solely on Prompt Engineering} Several studies have investigated the capability of \ac{llms} to generate educational questions aligned with cognitive levels, primarily by refining and optimizing the prompts given to the models. These approaches aim to guide \ac{llms} towards producing questions that meet specific educational objectives without any modification of the underlying model architecture.
\begin{itemize}
    \item \cite{blobstein_angel_2023} developed Angel, a tool that utilizes GPT-3.5 for generating questions at varying cognitive levels based on Bloom's Taxonomy. The system employs instructional prompts with a few-shot learning approach and textbook content to produce questions categorized by difficulty (easy, medium, hard). The evaluation of the generated questions and their answers was conducted using GPT-3.5 again as an evaluator with a \ac{cot} strategy, focusing on metrics such as \textit{Clarity, Relatedness, Importance,} and \textit{Answerability}. Human evaluation was also performed.
    % Gaps: limited ability to generate questions targeting high cognitive levels, control over question complexity and difficulty, create adequate QA pairs
    % How trying to solve: Angel --> LLM-based generating with advanced prompting (CoT, Few-shot, Bloom instruction) to generate QA pairs wird diverse difficulty levels, also targeting higher cognitive levels; via textbook
    % What not solved: potential was shown for llms in aqg, but bloom adherence not sufficient; future work: more samples needed, more llms for llm-as-evaluator to avoid llm bias
    \item \cite{scaria_how_2024} assessed the capability of \ac{llms}, including GPT-3.5 and GPT-4, to generate high-quality questions across different cognitive levels using zero-shot prompting. The CEFR level was also specified in the prompts. Two annotators evaluated the generated questions based on categories adapted from \cite{horbach_linguistic_2020}, where the \textit{Rephrase} category was modified to \textit{Bloom'sLevel} to assess adherence to the targeted cognitive level. The Inter-annotator agreement was measured using Cohen's Kappa \cite{cohen_coefficient_1960}. Their findings suggest that \ac{llms} can produce relevant and high-quality questions at diverse cognitive levels, indicating their utility for scalable educational content generation, with GPT-4 and GPT-3.5 being notably effective.
    % Gaps: creating pedagogically effective questions, especially at higher skill levels, is challenging and time-consuming; expert eval is necessary to understand the pedagogical aspects of machine generated questions
    % How trying to solve: 5 sota llms using zero-shot for relevant, high quality questions across blooms levels, >91% questions were relevant/high quality
    % What not solved: human expert ratings still needed --> can be time-consuming and subjective; questions at apply and create remained a challenge for most models
    \item \cite{elkins_how_2023} investigated steering question generation with InstructGPT \cite{ouyang_training_2022} by employing a combination of Bloom's Taxonomy and a difficulty-level taxonomy \cite{perez_automatic_2012}, including \textit{Beginner, Intermediate, Advanced}. The generated questions were assessed by 19 annotators (11 from biology and 8 from machine learning) on criteria including relevance, grammar, adherence to the taxonomy, answerability, and usefulness. Inter-annotator agreement was determined using Cohen's Kappa. The study concluded that five-shot prompting was more effective than zero-shot prompting for this task.
    % Gaps: recent work fails to show that real teachers judge the generated questions as sufficiently useful for the classroom setting; or if instead the questions have errors and/or pedagogically unhelpful content
    % How trying to solve: human eval study with experienced teachers --> question quality and usefulness assessment by instructgpt from given texts with 9 qtypes and 2 domains; high ratings for quality and usefulness --> empirical support for utility of llms in aqg 
    % What not solved: out of 612 candidate questions, there are 540 unique ones --> overlapping questions --> limitations in truly diverse outputs; using multiple llms; comparing machine-generated questions with human-authored ones
    \item \cite{maity_can_2025} explored the potential of \ac{llms} for \ac{aqg}, with a focus on Bloom's revised Taxonomy. Using the same evaluation metrics as \cite{elkins_how_2023}, the study compared simple zero-shot prompting against eight-shot prompting. 16 school teachers evaluated the generated questions, finding that eight-shot prompting improved the quality of educational questions. GPT-4-turbo was identified as the best performing model under both prompting conditions. 
    % Gaps: aqg is time-consuming; critial gap --> systematic assessment of the generated questions using frameworks e.g. bloom not fully integrated; and completeness + relevance are gaps
    % How trying to solve: generating a complete set of questions aligned with bloom via school textbooks, using zero-shot and eight-shot prompting; human eval by teachers; eight-shot improved quality of educational questions
    % What not solved: subjective human assessment
    \item \cite{al_faraby_analysis_2024} investigated the use of \ac{llms} for educational question generation with ChatGPT vs. LLaMA2-13B. The study focused on textbook-based questions and different prompting strategies: zero-shot, enhanced zero-shot, few-shot, and few-shot with \ac{cot}, with each version tested via 6 variations. Human evaluators (experts and crowdsourcing) were employed, revealing that experts preferred human-generated questions from textbooks, while crowdsourced evaluators illustrated comparable preferences for both human- and \ac{llm}-generated questions. The enhanced zero-shot approach outperformed the other methods. The evaluation included metrics such as \textit{Clarity, Alignment with Learning Objectives} (obtained from the textbooks' sections), \textit{Simulation to critical Thinking, Usefulness, Difficulty Level}, and \textit{Resemblance to Human-generated Questions}. Also, the inter-rater agreement was measured using Cohen's Kappa.
    % Gaps: despite advancements in llm ability to produce syntactically and semantically sound questions, quality and pedagogical effectiveness of llm-generated questions were not thoroughly evaluated; alignment with educational objectives and stimulating students' critical thinking unexplored
    % How trying to solve: compare chatgpt questions with human-expert generated questions from textbooks --> assess quality and pedagogical effectiveness
    % What not solved: hallucination needs further attention; integrate systems to verify questions against source texts; small number of annotators; more diverse questions needed and broader range of llms; forum questions...
    \vspace{9em}\pagebreak
    \item \cite{vu_chatgpt-based_2024} aimed to support educators in generating question banks in higher education using ChatGPT. Drawing inspiration from \cite{cavojsky_exploring_2023}, the authors established several prompt patterns for each question type (Multiple-Choice, True-False, Calculative Exercise). For instance, a combined prompt called RTCEF was used, which included the \ac{llm}'s role, the desired task, context, an example, and the output format. While noting the potential for supporting educators, the study highlighted issues with calculative exercises. These questions were often filtered out due to insufficient data, relying on the probability of word appearance rather than logical factors for generation. Their evaluation conducted a blind test where lecturers were best able to differentiate between human- and \ac{llm}-generated questions, while students' groups showed less accuracy.
    % Gaps: no in-depth study for llms' support for time-consuming task of testing and evaluating learners; evaluation of utility of llm-generated questions for testing real students is limited
    % How trying to solve: chatgpt-3.5 with interactive prompting patterns --> support higher education educators in generating question banks; eval via blind test with lecturers and students
    % What not solved: llms can lack logical factors --> issues in data for calculative exercises; links to other concepts lacking or just vague --> potentially limiting pedagogical effectiveness for deep learning
\end{itemize}

\object{Focusing solely on \ac{llm} Fine-Tuning} Another avenue of research involves fine-tuning pre-trained \ac{llms} on domain-specific or task-specific data to enhance their question generation capabilities. This approach aims to adapt the models to better understand educational contexts and produce more relevant and accurate questions.
\begin{itemize}
    \item \cite{duong-trung_bloomllm_2024} introduced BloomLLM, a fine-tuned GPT-3.5-turbo model, which was developed to generate educational questions with respect to Bloom's Taxonomy via descriptive zero-shot prompting. The fine-tuning was based on manually annotated questions considering the cognitive levels of Bloom's Taxonomy. 46 master's students were asked to tell which questions were preferred, and the results show that BloomLLM's questions have a higher relevance across all levels of Bloom's Taxonomy in contrast to the successor GPT-4.
    % Gaps: foundational llms struggle with bloom alignment for personalized qgen in higher education; lack of interdependence between cognitive levels, failing at semantic connections; poor apply level performance; unrealistic higher-level questions
    % How trying to solve: BloomLLM, ft of gpt-3.5-turbo on manually annotated questions across bloom levels; enhance generation across taxonomy spectrum; questionnaire with 46 master's students to evaluate question preferences across blooms levels
    % What not solved: /
    \item \cite{lamsiyah_fine-tuning_2024} propose RLLM-EduQG, a system that consists of a fine-tuned Google FLAN-T5 model for \ac{aqg}. From text passages, the model generates questions and answers. The authors employed two reward functions to guide the model's learning process: the BLEU metric and a semantic reward function. RLLM-EduQG was evaluated using several simple metrics, including \textit{BLEU, F1, ROUGE, Perplexity,} and \textit{Diversity}, indicating a high predictive and linguistic quality.
    % Gaps: aqg focuses on generic domains --> need for educational domain; traditional ft methods have issues with exposure bias / training/testing inconsistency; prior RL often overlooks metrics like semantic constraints.
    % How trying to solve: combine llm with RL, optimizing eval metrics with semantic sim; address exposure bias in answer-unaware model
    % What not solved: human eval absence besides metric-based eval on datasets; planned to fine-tune other models besides FLAN-T5
\end{itemize}

\object{Integrated Systems\,/\,Hybrid Architectures} Researchers have also developed more complex systems that integrate \ac{llms} with other components or employ multiple \ac{llms} in a coordinated fashion. These hybrid architectures often aim to leverage the strengths of different techniques to achieve more robust and diverse question generation.
\begin{itemize}
    \item \cite{cheng_treequestion_2024} proposed TreeQuestion, a system utilizing GPT-3.5-turbo for generating \ac{mcqs}. Their approach follows an \enquote{Explore-Validate-Generate} pattern: initially, an \ac{llm} generates knowledge graphs in a zero-shot setting to explore concepts, which educators can then validate. Subsequently, TreeQuestion generates questions aiming for alignment with Bloom's Taxonomy levels. A software interface allows educators to select Bloom's levels and set question difficulty. Evaluation by 10 teachers and 96 students indicated that the generated \ac{mcqs} were comparable in overall quality to human-generated open-ended questions. The study also discussed limitations, such as generating questions requiring complex cognitive processes like intricate reasoning. The system used Bloom-based, zero-shot prompts and focused on time-saving for educators.
    % Gaps: llms make errors and redundancy when generating mcqs; existing aqg struggles with higher bloom levels --> low quality and limited diversity
    % How trying to solve: TreeQuestion, a system employing gpt-3.5-turbo where teachers validate generated knowledge graphs before generating mcqs; detailed bloom taxonomy descriptions in prompts to easily specify and incorporate desired difficulty levels
    % What not solved: designed and evaluated for the purpose of assessing conceptual learning outcomes and not intended to support other educational goals for questions (e.g. creative thinking skills)
    \item \cite{doughty_comparative_2024} conducted a comparative study of \ac{mcqs} generated by GPT-4 versus those created by humans. The authors developed a system that generates \ac{mcqs} from course context, primarily focusing on learning objectives. These objectives were aligned with Bloom's Taxonomy levels using a fine-tuned BERT classifier. Human evaluation indicated that GPT-4 produced \ac{mcqs} comparable to human-generated ones in terms of clarity, distractor quality, and learning objective coverage. The system utilized descriptive and instructional few-shot prompts that also explained the Bloom's levels, without providing any context for the \ac{llm}-based questions.
    % Gaps: high quality mcq generation is effortful; llms for programming mcqs, especially generating them from learning objectives rather than text, is less explored
    % How trying to solve: gpt-4 system for mcqs from high-level course context and module-level learning objectives for programming classes; eval of quality and LO alignment; bloom and question types
    % What not solved: bias from human evaluators; only programming questions; consider the alignment between an MCQ and the learning content; study llm capabilities with different parameters; create entire tests/quizzes
    \item \cite{zhuge_twinstar_2025} developed TwinStar, a novel system employing a dual-\ac{llm} engine for educational question generation. The system utilizes two-shot prompting based on multiple rounds, recognizing that advanced \ac{llms} like GPT-4 struggle with precise adherence to Bloom's Taxonomy levels. Therefore, the authors fine-tuned ChatGLM2-6B \cite{glm_chatglm_2024} and LLaMA2-13B \cite{touvron_llama_2023-1}, with LLaMA2-13B demonstrating superior performance. TwinStar operates through an iterative process involving two engines: one for question generation and another for evaluation. The evaluation model was fine-tuned on 1000 questions to better recognize Bloom level features. If a generated question aligns with the target Bloom's level and passes a redundancy check, it is accepted. Otherwise, the generation model refines the question based on feedback from the evaluation model to achieve the desired cognitive level. Experiments demonstrated TwinStar's superior performance in Bloom's Taxonomy adherence compared to \ac{llms} such as Bard, Claude 3, and GPT-4.
    % Gaps: high quality test question generation with specified knowledge points and cognitive levels is difficult; current llms lack focus on cognitive levels for specific knowledge points; achieving desired intelligence requires many resources for training such domain-specific models
    % How trying to solve: TwinStar, dual llm engine with fine-tuned lightweight models and iterative prompting to enhance cognitive-level adherence and knowledge relevance
    % What not solved: Future work --> "such as conducting in-depth studies on the impacts of reinforcement learning and modal distillation on the application of LLMs in professional domains and exploring the possibilities of using a multiple-agent AI engine to obtain collective intelligence on a network of mobile devices."
    % \item \cite{yang_heuristic_2024} addressed hallucination, which is unacceptable in education. Moreover, the authors proposed a system based on \ac{rag}, leveraging a knowledge graph constructed from course content to supply domain-specific knowledge. For knowledge retrieval, a fine-tuned BERT model was employed, and GLM-4 \cite{glm_chatglm_2024} was the utilized \ac{llm} for question generation. The Prompts included task descriptions, examples, knowledge paths, and the target Bloom cognitive levels. Ultimately, the evaluation focuses on aspects like \textit{Perplexity, Diversity} and human evaluation with 20 students based on a certain statement list of 12 items.
    % Gaps: traditional education and intelligent tutoring systems lack the cultivation of students’ independent learning abilities; qg focuses on single questions instead of multiple related questions; reliance on llms can cause hallucination, which is unacceptable in education; "Question generation in education is a knowledge-intensive task, and the content generation needs the guidance of profes-sional domain knowledge."
    % How trying to solve: personalized knowledge paths from a knowledge graph with RAG to guide GLM-4 in generating controllable, heuristic question sequences for self-learning --> addressing hallucination
    % What not solved: optimizing response time, integrating approach across multiple disciplines; incorporating more student learning behaviors beyond test records and enhancing language features; refining the evaluation mechanism beyond the current metrics
    \item \cite{hang_mcqgen_2024} presented MCQGen, a system that generates \ac{mcqs} for personalized learning by using GPT-4. They employed \ac{rag} and advanced prompting techniques, drawing on an external knowledge base. The approach leveraged the advanced semantic understanding and summarization capabilities of GPT-4. Both human- and \ac{llm}-based evaluators were used to assess the generated questions to provide a comprehensive assessment. Mixed outcomes were observed between e.g.\ LLaMA2 and GPT-3.5 versus human lecturers. While utilizing \ac{cot} prompting with self-refinement, the evaluation focused on \textit{Grammar, Answerability, Diversity, Complexity,} and \textit{Relevance}. Diversity and answerability lacked in both evaluation methods, suggesting that prompt optimization and expanding the knowledge base could address these limitations. 
    % Gaps: efficient and effective mcq generation is needed in modern teaching; manual mcqgen is resource-intensive and time-consuming; automated systems depend heavily on model understanding and still return ineffective distractors or even errors
    % How trying to solve: mcqgen with GPT-4, using RAG and advanced prompting (cot, self-refinement) via an external knowledge base (based on instructor and student mcqs) --> automate personalized mcq creation
    % What not solved: diversity and complexity were sometimes limited, possibly due to prompt engineering and the knowledge base; only 1 model (gpt-4), only computer science, only english language
\end{itemize}

\vspace{13em}\pagebreak

\object{Research Gaps addressed by this Thesis} ...

\newtodo{Maybe the texts above need a little refinement, so that the gaps are more visible.}

\newtodo{Create proper research gaps text, what do i want to address with my thesis?}

\begin{itemize}
    \item outdated models or small language models used, even in current papers
    \item uprising reasoning/thinking capabilities were not considered
    \item "A significant research gap exists regarding the creation of
cognitively challenging questions, particularly the effective implementation and scoring of questions
aligned with higher levels of Bloom's Taxonomy. While LLMs such as ChatGPT [Ope22] are good
at generating questions for lower cognitive levels, they have limitations at the higher levels, often
generating oversimplified or unrealistic questions [DWK24; Elk+23; SDS24]"
    \item what else?
\end{itemize}